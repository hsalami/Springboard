{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Loading-the-Data-and-Required-modules\" data-toc-modified-id=\"Loading-the-Data-and-Required-modules-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading the Data and Required modules</a></span></li><li><span><a href=\"#Cramer's-V-Coefficient-of-the-Features\" data-toc-modified-id=\"Cramer's-V-Coefficient-of-the-Features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Cramer's V Coefficient of the Features</a></span></li><li><span><a href=\"#Encoding-Categorical-Features\" data-toc-modified-id=\"Encoding-Categorical-Features-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Encoding Categorical Features</a></span></li><li><span><a href=\"#Models'-Training-without-Undersampling\" data-toc-modified-id=\"Models'-Training-without-Undersampling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Models' Training without Undersampling</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoding\" data-toc-modified-id=\"One-Hot-Encoding-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>One Hot Encoding</a></span></li><li><span><a href=\"#Binary-Encoding\" data-toc-modified-id=\"Binary-Encoding-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Binary Encoding</a></span></li><li><span><a href=\"#Leave-One-Out-Encoding\" data-toc-modified-id=\"Leave-One-Out-Encoding-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Leave-One-Out Encoding</a></span></li></ul></li><li><span><a href=\"#Models'-Training-with-Undersampling\" data-toc-modified-id=\"Models'-Training-with-Undersampling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Models' Training with Undersampling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Random-Forest-Classifier\" data-toc-modified-id=\"Balanced-Random-Forest-Classifier-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Balanced Random Forest Classifier</a></span></li><li><span><a href=\"#Training-with-Undersampling\" data-toc-modified-id=\"Training-with-Undersampling-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Training with Undersampling</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoding\" data-toc-modified-id=\"One-Hot-Encoding-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>One Hot Encoding</a></span></li><li><span><a href=\"#Binary-Encoding\" data-toc-modified-id=\"Binary-Encoding-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Binary Encoding</a></span></li><li><span><a href=\"#Leave-One-Out-Encoding\" data-toc-modified-id=\"Leave-One-Out-Encoding-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Leave-One-Out Encoding</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li></ul></li><li><span><a href=\"#Fine-Tuning-and-Model-Selection\" data-toc-modified-id=\"Fine-Tuning-and-Model-Selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Fine-Tuning and Model Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-of-Selected-Model\" data-toc-modified-id=\"Performance-of-Selected-Model-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Performance of Selected Model</a></span></li><li><span><a href=\"#Focus-on-the-Known-Primary-Contributory-Cause\" data-toc-modified-id=\"Focus-on-the-Known-Primary-Contributory-Cause-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Focus on the Known Primary Contributory Cause</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Overview\n",
    "\n",
    "After we have explored the data visually and statistically, we are now ready to build a predictive model from the features of crashes in order to predict the crash type: Injury or No Injury crash. In this report, we go over the steps done to build the predictive model. We address first the different encoding schemes we tried for categorical variables, we then train different models and compare their performance. Since we have imbalances in the data (around 78% for no Injury crashes and 22% for Injury crashes), we compute the performance of the trained models in terms of their precision, recall and F1 scores. Since the performance of the trained models can be affected by the imbalances in the data, we also consider undersampling for the No Injury crash type and train the models accordingly. We then choose the best model and compute its performance on the test set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data and Required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the required modules, and we load the crashes' features and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score as accuracy, precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall, f1_score as f1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"crash_features.csv\")\n",
    "Y = pd.read_csv(\"crash_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the two crash types as follows:\n",
    "- \"INJURY AND / OR TOW DUE TO CRASH\": 1\n",
    "- \"NO INJURY / DRIVE AWAY\": 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (Y[\"CRASH_TYPE\"] == \"INJURY AND / OR TOW DUE TO CRASH\").values.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into two sets: 80% for training and 20% for testing. We use the training set to train multiple models, compare between their performance and select the final model using cross validation. On the other hand, we use the test set to test the final model and report its predicted performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "    X.iloc[:, 1:], Y, train_size=0.8, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cramer's V Coefficient of the Features\n",
    "\n",
    "Based on our previous data exploration, we noticed that some features have weak relationship with the crash type. To check again the association between the features and the crash type, we compute the Cramer's V coefficients for each feature (using the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cramer's V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>POSTED_SPEED_LIMIT</td>\n",
       "      <td>0.126719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TRAFFIC_CONTROL_DEVICE</td>\n",
       "      <td>0.119743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DEVICE_CONDITION</td>\n",
       "      <td>0.106610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WEATHER_CONDITION</td>\n",
       "      <td>0.056911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LIGHTING_CONDITION</td>\n",
       "      <td>0.131978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FIRST_CRASH_TYPE</td>\n",
       "      <td>0.355483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TRAFFICWAY_TYPE</td>\n",
       "      <td>0.165673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ALIGNMENT</td>\n",
       "      <td>0.055291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ROADWAY_SURFACE_COND</td>\n",
       "      <td>0.062530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PRIM_CONTRIBUTORY_CAUSE</td>\n",
       "      <td>0.305199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRASH_HOUR</td>\n",
       "      <td>0.140193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRASH_DAY_OF_WEEK</td>\n",
       "      <td>0.039228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRASH_MONTH</td>\n",
       "      <td>0.023056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AREAS</td>\n",
       "      <td>0.111112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Cramer's V\n",
       "POSTED_SPEED_LIMIT         0.126719\n",
       "TRAFFIC_CONTROL_DEVICE     0.119743\n",
       "DEVICE_CONDITION           0.106610\n",
       "WEATHER_CONDITION          0.056911\n",
       "LIGHTING_CONDITION         0.131978\n",
       "FIRST_CRASH_TYPE           0.355483\n",
       "TRAFFICWAY_TYPE            0.165673\n",
       "ALIGNMENT                  0.055291\n",
       "ROADWAY_SURFACE_COND       0.062530\n",
       "PRIM_CONTRIBUTORY_CAUSE    0.305199\n",
       "CRASH_HOUR                 0.140193\n",
       "CRASH_DAY_OF_WEEK          0.039228\n",
       "CRASH_MONTH                0.023056\n",
       "AREAS                      0.111112"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = Xtrain.columns\n",
    "\n",
    "\n",
    "def cram(chi2, table):\n",
    "    ntot = table.sum().sum()\n",
    "    nrow, ncol = table.shape\n",
    "    phi_sqr = chi2/ntot\n",
    "    phi_sqr_tilde = max(0, phi_sqr-(nrow-1)*(ncol-1)/(ntot-1))\n",
    "    ncol_tilde = ncol-((ncol-1)**2)/((ntot-1))\n",
    "    nrow_tilde = nrow-((nrow-1)**2)/((ntot-1))\n",
    "    cramer = np.sqrt(phi_sqr_tilde/min(ncol_tilde-1, nrow_tilde-1))\n",
    "    return cramer\n",
    "\n",
    "\n",
    "# Initialize an empty array that will have the results for each feature\n",
    "values = np.empty(shape=[1, len(colnames)])\n",
    "col = 0\n",
    "\n",
    "# Loop over the features and save cramer's v coefs\n",
    "for colname in colnames:\n",
    "    table = pd.crosstab(Ytrain, Xtrain[colname])\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    cramer = cram(chi2, table)\n",
    "    values[0][col] = cramer\n",
    "    col = col + 1\n",
    "\n",
    "# Create a dataframe for the results\n",
    "results = pd.DataFrame(values)\n",
    "results.columns = colnames\n",
    "results.rename(index={0: \"Cramer's V\"}, inplace=True)\n",
    "results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some crash features have low Cramer's V coefficients (crash_month: 0.023 and crash_day_of_week: 0.039). Although low values of Cramer's V coefficients reflect weak relationship between the feature and the crash type, we decide for now to keep all the features and to not remove any column, especially that we have only 14 features in total, while the number of samples in the training set is around 240k samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Features\n",
    "\n",
    "The data consists of two types of categorical features:\n",
    "- nominal: which we need to convert to numerical features;\n",
    "- ordinal: which are the speed limit, crash hour, crash month and crash day of the week; since these variables are already in numerical form, we do not modify them in the preprocessing step.\n",
    "\n",
    "Before we start training the model, we convert the entries of the nominal categorical features into numerical entries. One of the basic encoding scheme is one-hot encoding, which replaces a categorical feature with a number of columns equal to the number of levels or categories within the considered feature. Each level is then mapped to a series of 0 and 1, where the corresponding column of the level takes 1 and the remaining columns take 0. One of the main drawback of this scheme is that it increases the number of features in the data. Another encoding scheme that can replace a given categorical feature with a less number of columns is binary encoding, where each level of a given categorical feature is mapped to a binary number. There are also other types of encoding that replace a categorical feature with only one column, where each level of the given feature is mapped to a number computed based on its frequency. Some other schemes embed the target variable in the computation. For instance, target encoding is an encoding scheme that replaces features with a combination of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data. Leave-one-out is \"very similar to target encoding but excludes the current rowâ€™s target when calculating the mean target for a level One of these schemes\", which makes this scheme less prone to overfitting and response leakage. For reference, check [here](https://contrib.scikit-learn.org/categorical-encoding/leaveoneout.html) and [here](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02) \n",
    "\n",
    "We next consider these different schemes of encoding, namely one hot, binary and leave-one-out encodings, and try them with multiple training models to choose at the end the best combination. We create two versions of the training set based on binary and one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# names of columns to encode\n",
    "col_names = ['TRAFFIC_CONTROL_DEVICE', 'DEVICE_CONDITION', 'WEATHER_CONDITION',\n",
    "             'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE', 'TRAFFICWAY_TYPE',\n",
    "             'ALIGNMENT', 'ROADWAY_SURFACE_COND', 'PRIM_CONTRIBUTORY_CAUSE', 'AREAS']\n",
    "\n",
    "# use binary encoding to encode two categorical features\n",
    "encoder_bin = ce.BinaryEncoder(cols=col_names).fit(Xtrain)\n",
    "encoder_one_hot = ce.OneHotEncoder(cols=col_names).fit(Xtrain)\n",
    "\n",
    "# transform the dataset\n",
    "Xtr_bin = encoder_bin.transform(Xtrain)\n",
    "Xtr_one_hot = encoder_one_hot.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to consider the leave-one-out encoding scheme, but since this encoding uses the target variable (type of crash) while encoding, we are going to encode the set of the data folds that we are going to train while doing cross-validation. It will become clearer in the sequel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, we outline the next steps:\n",
    "- For each encoding scheme, we train the following set of training models: logistic regression, linear support vector machine (for linear models), random forest, Ada Boost and gradient tree boosting (for tree based models) and multinomial Naive Bayes, keeping their default parameters. The performance of the trained models is computed in terms of F1-score using 5-fold cross validation. \n",
    "- We then address the imbalances in the data by considering the model: BalancedRandomForestClassifier from the imblearn library and by also doing undersampling for the No Injury crashes and then training the same set of regular models we mentioned in the previous step.\n",
    "- We finally select the best combination of encoding and training models, we re-fit the final model using the whole training set and then test the performance of the final model using the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models' Training without Undersampling\n",
    "\n",
    "We first train various classifiers while keeping the imbalanced data as is. We will try the three different encoding schemes and summarize at the end the obtained results in a table. We first define the functions that we are going to use to train the training set and report the performance by means of cross-validation. The use of cross-validation is important to avoid overfitting and to correctly compare the performance between the different trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define two functions to use in our analysis. The first function takes as input the model to train, the features, the target variable and the number of folds for cross-validation. We are going to use this function with the data encoded using one-hot and binary encoding schemes. This function uses cross-validation to train the model of interest and returns the means of the following metrics: accuracy, precision, recall and F1-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def cv_score(clf, x, y, cv, scale=False):\n",
    "    result = np.zeros((cv, 4))\n",
    "    i = 0\n",
    "    # split data into train/test groups, 'cv' times\n",
    "    for train, test in StratifiedKFold(cv).split(x, y):\n",
    "        x_train = x[train]\n",
    "        x_test = x[test]\n",
    "        if (scale == True):\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_train, y[train])  # fit\n",
    "        ypred = clf.predict(x_test)\n",
    "        result[i, 0] = accuracy(ypred, y[test])\n",
    "        result[i, 1] = precision(ypred, y[test])\n",
    "        result[i, 2] = recall(ypred, y[test])\n",
    "        result[i, 3] = f1(ypred, y[test])\n",
    "        i = i+1\n",
    "    return np.mean(result, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous function, this second function also performs cross-validation to train the desired model, but first encodes the folds that are intended for training using leave-one-out scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_LOO(clf, x, y, cv, colnames, scale=False):\n",
    "    result = np.zeros((cv, 4))\n",
    "    i = 0\n",
    "    # split data into train/test groups, 'cv' times\n",
    "    for train, test in StratifiedKFold(cv).split(x, y):\n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder(cols=colnames).fit(\n",
    "            x.iloc[train, ], y[train])\n",
    "        x_trans = encoder.transform(x.iloc[train])\n",
    "        x_test = encoder.transform(x.iloc[test])\n",
    "        if (scale == True):\n",
    "            scaler = StandardScaler()\n",
    "            x_trans = scaler.fit_transform(x_trans)\n",
    "            x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_trans, y[train])  # fit\n",
    "        ypred = clf.predict(x_test)\n",
    "        result[i, 0] = accuracy(ypred, y[test])\n",
    "        result[i, 1] = precision(ypred, y[test])\n",
    "        result[i, 2] = recall(ypred, y[test])\n",
    "        result[i, 3] = f1(ypred, y[test])\n",
    "        i = i + 1\n",
    "    return np.mean(result, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics Used\n",
    "Note that we are not only returning the accuracy of the trained model, but also its precision, recall and F1-score. As we previously mentioned, the metric of accuracy won't be helpful because of the imbalances in the data, this is why we look at different metrics. Since we are interested in detecting the Injury crashes, the metric recall measures how well the model is able to detect the Injury crash (minority class) by computing the true positive rate, and the metric precision indicates how much of the returned 1 labels do actually belong to Injury crashes. F1 score is the harmonic mean between precision and recall, and it is the metric we are going to use in deciding on the final model.\n",
    "\n",
    "We are now ready to train the models for each encoding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "We start with the one hot encoding, and we use the classifiers with their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = SGDClassifier(loss='log')\n",
    "linear_svm = SGDClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "ada_boost = AdaBoostClassifier()\n",
    "grad_boosting = GradientBoostingClassifier()\n",
    "naive_bayes = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the models. Note that for linear models we made sure to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lg = cv_score(logistic_reg, Xtr_one_hot.values,\n",
    "                     Ytrain, cv=5, scale=True)\n",
    "result_lsvm = cv_score(linear_svm, Xtr_one_hot.values,\n",
    "                       Ytrain, cv=5, scale=True)\n",
    "result_rf = cv_score(random_forest, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_aboost = cv_score(ada_boost, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_gboost = cv_score(grad_boosting, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_nb = cv_score(naive_bayes, Xtr_one_hot.values, Ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the result into the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One Hot Encoding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.798314</td>\n",
       "      <td>0.431709</td>\n",
       "      <td>0.562788</td>\n",
       "      <td>0.488599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.814306</td>\n",
       "      <td>0.319160</td>\n",
       "      <td>0.679115</td>\n",
       "      <td>0.433918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.807241</td>\n",
       "      <td>0.251848</td>\n",
       "      <td>0.686132</td>\n",
       "      <td>0.368242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.815023</td>\n",
       "      <td>0.353664</td>\n",
       "      <td>0.659661</td>\n",
       "      <td>0.460459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.816063</td>\n",
       "      <td>0.304723</td>\n",
       "      <td>0.702781</td>\n",
       "      <td>0.425097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.816224</td>\n",
       "      <td>0.263173</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.389947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall        F1\n",
       "One Hot Encoding                                            \n",
       "Naive Bayes          0.798314   0.431709  0.562788  0.488599\n",
       "Logistic Regression  0.814306   0.319160  0.679115  0.433918\n",
       "Linear SVM           0.807241   0.251848  0.686132  0.368242\n",
       "Random Forest        0.815023   0.353664  0.659661  0.460459\n",
       "Ada Boost            0.816063   0.304723  0.702781  0.425097\n",
       "Gradient Boosting    0.816224   0.263173  0.752400  0.389947"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    [result_nb, result_lg, result_lsvm, result_rf, result_aboost, result_gboost])\n",
    "results.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results.rename(index={0: \"Naive Bayes\", 1: \"Logistic Regression\", 2: \"Linear SVM\",\n",
    "                      3: \"Random Forest\", 4: \"Ada Boost\",\n",
    "                      5: \"Gradient Boosting\"}, inplace=True)\n",
    "results.index.name = \"One Hot Encoding\"\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for all of the models, the accuracy is around 80%, which is expected because of the imbalances in the data. Now if we look at the values of precision and recall, we notice that while some models have good recall, we see that that their precision is much lower. This is reflected by the F1-scores that are below 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the models with the features being binary encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lg = cv_score(logistic_reg, Xtr_bin.values, Ytrain, cv=5, scale=True)\n",
    "result_lsvm = cv_score(linear_svm, Xtr_bin.values, Ytrain, cv=5, scale=True)\n",
    "result_rf = cv_score(random_forest, Xtr_bin.values, Ytrain, cv=5)\n",
    "result_aboost = cv_score(ada_boost, Xtr_bin.values, Ytrain, cv=5)\n",
    "result_gboost = cv_score(grad_boosting, Xtr_bin.values, Ytrain, cv=5)\n",
    "result_nb = cv_score(naive_bayes, Xtr_bin.values, Ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are grouped as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary Encoding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.775947</td>\n",
       "      <td>0.188108</td>\n",
       "      <td>0.494881</td>\n",
       "      <td>0.272592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>0.160535</td>\n",
       "      <td>0.572644</td>\n",
       "      <td>0.249994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.776799</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.500554</td>\n",
       "      <td>0.006567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.811426</td>\n",
       "      <td>0.319868</td>\n",
       "      <td>0.659977</td>\n",
       "      <td>0.430882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.787542</td>\n",
       "      <td>0.162181</td>\n",
       "      <td>0.586988</td>\n",
       "      <td>0.254136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.810177</td>\n",
       "      <td>0.246975</td>\n",
       "      <td>0.716941</td>\n",
       "      <td>0.367374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall        F1\n",
       "Binary Encoding                                             \n",
       "Naive Bayes          0.775947   0.188108  0.494881  0.272592\n",
       "Logistic Regression  0.785885   0.160535  0.572644  0.249994\n",
       "Linear SVM           0.776799   0.003309  0.500554  0.006567\n",
       "Random Forest        0.811426   0.319868  0.659977  0.430882\n",
       "Ada Boost            0.787542   0.162181  0.586988  0.254136\n",
       "Gradient Boosting    0.810177   0.246975  0.716941  0.367374"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bin = pd.DataFrame(\n",
    "    [result_nb, result_lg, result_lsvm, result_rf, result_aboost, result_gboost])\n",
    "results_bin.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results_bin.rename(index={0: \"Naive Bayes\", 1: \"Logistic Regression\", 2: \"Linear SVM\",\n",
    "                          3: \"Random Forest\", 4: \"Ada Boost\",\n",
    "                          5: \"Gradient Boosting\"}, inplace=True)\n",
    "results_bin.index.name = \"Binary Encoding\"\n",
    "results_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the models performed worse with the binary encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the models with the features being encoded with the leave-one-out encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lg = cv_score_LOO(logistic_reg, Xtrain, Ytrain,\n",
    "                         colnames=col_names, cv=5, scale=True)\n",
    "result_lsvm = cv_score_LOO(linear_svm, Xtrain, Ytrain,\n",
    "                           cv=5, colnames=col_names, scale=True)\n",
    "result_rf = cv_score_LOO(random_forest, Xtrain, Ytrain,\n",
    "                         colnames=col_names, cv=5)\n",
    "result_aboost = cv_score_LOO(\n",
    "    ada_boost, Xtrain, Ytrain, colnames=col_names, cv=5)\n",
    "result_gboost = cv_score_LOO(\n",
    "    grad_boosting, Xtrain, Ytrain, colnames=col_names, cv=5)\n",
    "result_nb = cv_score_LOO(naive_bayes, Xtrain, Ytrain, colnames=col_names, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leave-One-Out Encoding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.778478</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.736858</td>\n",
       "      <td>0.022786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.815354</td>\n",
       "      <td>0.314403</td>\n",
       "      <td>0.691126</td>\n",
       "      <td>0.431386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.236390</td>\n",
       "      <td>0.745456</td>\n",
       "      <td>0.356384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.815056</td>\n",
       "      <td>0.367837</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.470277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.816426</td>\n",
       "      <td>0.320444</td>\n",
       "      <td>0.691529</td>\n",
       "      <td>0.437905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.819663</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.703636</td>\n",
       "      <td>0.450841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy  Precision    Recall        F1\n",
       "Leave-One-Out Encoding                                         \n",
       "Naive Bayes             0.778478   0.011573  0.736858  0.022786\n",
       "Logistic Regression     0.815354   0.314403  0.691126  0.431386\n",
       "Linear SVM              0.811000   0.236390  0.745456  0.356384\n",
       "Random Forest           0.815056   0.367837  0.651800  0.470277\n",
       "Ada Boost               0.816426   0.320444  0.691529  0.437905\n",
       "Gradient Boosting       0.819663   0.331687  0.703636  0.450841"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_loo = pd.DataFrame(\n",
    "    [result_nb, result_lg, result_lsvm, result_rf, result_aboost, result_gboost])\n",
    "results_loo.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results_loo.rename(index={0: \"Naive Bayes\", 1: \"Logistic Regression\",\n",
    "                          2: \"Linear SVM\", 3: \"Random Forest\",\n",
    "                          4: \"Ada Boost\", 5: \"Gradient Boosting\"}, inplace=True)\n",
    "results_loo.index.name = \"Leave-One-Out Encoding\"\n",
    "results_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ensemble methods (random forest, ada-boost and gradient boosting) have better F1-score with the leave-one-out encoding scheme than with the one-hot encoding. Logistic regression as well as linear SVM performed the same in terms of F1-score in both encoding scheme (one-hot and leave-one-out).\n",
    "\n",
    "With the presence of imbalances in the data, we saw that the models did not perform very well in terms of F1-score. Let us now check if we balance the data and train the models with the rebalanced data, how the performance will be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models' Training with Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the imbalances in the data, we are going to first use the balanced random Forest classifier model, which internally handles the imbalances in the data. We are also going to train the same set of models we trained previously, but after undersampling the data that belong to the majority class (No Injury crashes). We are choosing the undersampling technique because we have enough data to sample from.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Classifier\n",
    "We start with the balanced random forest classifier. We try it with both encoding schemes: one-hot and binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_rf = BalancedRandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_balancedrf_onehot = cv_score(\n",
    "    balanced_rf, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_balancedrf_bin = cv_score(balanced_rf, Xtr_bin.values, Ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Random Forest</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>One Hot</td>\n",
       "      <td>0.741213</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.544806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Binary</td>\n",
       "      <td>0.728644</td>\n",
       "      <td>0.696034</td>\n",
       "      <td>0.432888</td>\n",
       "      <td>0.533789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy  Precision    Recall        F1\n",
       "Balanced Random Forest                                         \n",
       "One Hot                 0.741213   0.693878  0.448465  0.544806\n",
       "Binary                  0.728644   0.696034  0.432888  0.533789"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_blrf = pd.DataFrame([result_balancedrf_onehot, result_balancedrf_bin])\n",
    "results_blrf.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results_blrf.rename(index={0: \"One Hot\", 1: \"Binary\"}, inplace=True)\n",
    "results_blrf.index.name = \"Balanced Random Forest\"\n",
    "results_blrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that with this balanced version of random forest, the accuracy and recall scores dropped, but the precision and F1 scores both increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Undersampling\n",
    "\n",
    "We now consider undersampling for the No Injury crashes. We first redefine the cross validation functions to take into account undersampling. For each split of the training set, we first perform undersampling on the k-1 folds and then train the model on the balanced data and then test on the k-th fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_undersample(clf, x, y, cv, scale=False):\n",
    "    result = np.zeros((cv, 4))\n",
    "    i = 0\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    # split data into train/test groups, 'cv' times\n",
    "    for train, test in StratifiedKFold(cv).split(x, y):\n",
    "        x_train, y_train = rus.fit_resample(x[train], y[train])\n",
    "        x_test = x[test]\n",
    "        if (scale == True):\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_train, y_train)  # fit\n",
    "        ypred = clf.predict(x_test)\n",
    "        result[i, 0] = accuracy(ypred, y[test])\n",
    "        result[i, 1] = precision(ypred, y[test])\n",
    "        result[i, 2] = recall(ypred, y[test])\n",
    "        result[i, 3] = f1(ypred, y[test])\n",
    "        i = i+1\n",
    "    return np.mean(result, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second function takes into account undersampling and leave one-out encoding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_LOO_undersample(clf, x, y, cv, colnames, scale=False):\n",
    "    result = np.zeros((cv, 4))\n",
    "    i = 0\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    # split data into train/test groups, 'cv' times\n",
    "    for train, test in StratifiedKFold(cv).split(x, y):\n",
    "        x_train, y_train = rus.fit_resample(x.iloc[train], y[train])\n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder(cols=colnames).fit(\n",
    "            x_train, y_train)\n",
    "        x_trans = encoder.transform(x_train)\n",
    "        x_test = encoder.transform(x.iloc[test])\n",
    "        if (scale == True):\n",
    "            scaler = StandardScaler()\n",
    "            x_trans = scaler.fit_transform(x_trans)\n",
    "            x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_trans, y_train)  # fit\n",
    "        ypred = clf.predict(x_test)\n",
    "        result[i, 0] = accuracy(ypred, y[test])\n",
    "        result[i, 1] = precision(ypred, y[test])\n",
    "        result[i, 2] = recall(ypred, y[test])\n",
    "        result[i, 3] = f1(ypred, y[test])\n",
    "        i = i + 1\n",
    "    return np.mean(result, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "We start with the one hot encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lg = cv_score_undersample(\n",
    "    logistic_reg, Xtr_one_hot.values, Ytrain, cv=5, scale=True)\n",
    "result_lsvm = cv_score_undersample(\n",
    "    linear_svm, Xtr_one_hot.values, Ytrain, cv=5, scale=True)\n",
    "result_rf = cv_score_undersample(\n",
    "    random_forest, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_aboost = cv_score_undersample(\n",
    "    ada_boost, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_gboost = cv_score_undersample(\n",
    "    grad_boosting, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "result_nb = cv_score_undersample(naive_bayes, Xtr_one_hot.values, Ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One Hot Encoding - Undersampling</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.707431</td>\n",
       "      <td>0.683919</td>\n",
       "      <td>0.407411</td>\n",
       "      <td>0.510633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.724030</td>\n",
       "      <td>0.676988</td>\n",
       "      <td>0.425774</td>\n",
       "      <td>0.522726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>0.659868</td>\n",
       "      <td>0.434885</td>\n",
       "      <td>0.524229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.724184</td>\n",
       "      <td>0.688775</td>\n",
       "      <td>0.426927</td>\n",
       "      <td>0.527120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.734721</td>\n",
       "      <td>0.671885</td>\n",
       "      <td>0.438476</td>\n",
       "      <td>0.530635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.733068</td>\n",
       "      <td>0.679161</td>\n",
       "      <td>0.436962</td>\n",
       "      <td>0.531776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Accuracy  Precision    Recall        F1\n",
       "One Hot Encoding - Undersampling                                         \n",
       "Naive Bayes                       0.707431   0.683919  0.407411  0.510633\n",
       "Logistic Regression               0.724030   0.676988  0.425774  0.522726\n",
       "Linear SVM                        0.732649   0.659868  0.434885  0.524229\n",
       "Random Forest                     0.724184   0.688775  0.426927  0.527120\n",
       "Ada Boost                         0.734721   0.671885  0.438476  0.530635\n",
       "Gradient Boosting                 0.733068   0.679161  0.436962  0.531776"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    [result_nb, result_lg, result_lsvm, result_rf, result_aboost, result_gboost])\n",
    "results.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results.rename(index={0: \"Naive Bayes\", 1: \"Logistic Regression\",\n",
    "                      2: \"Linear SVM\", 3: \"Random Forest\",\n",
    "                      4: \"Ada Boost\", 5: \"Gradient Boosting\"}, inplace=True)\n",
    "results.index.name = \"One Hot Encoding - Undersampling\"\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoding\n",
    "We now try undersmapling with the binary encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lg = cv_score_undersample(\n",
    "    logistic_reg, Xtr_bin.values, Ytrain, cv=5, scale=True)\n",
    "result_lsvm = cv_score_undersample(\n",
    "    linear_svm, Xtr_bin.values, Ytrain, cv=5, scale=True)\n",
    "result_rf = cv_score_undersample(random_forest, Xtr_bin.values, Ytrain, cv=5)\n",
    "result_aboost = cv_score_undersample(ada_boost, Xtr_bin.values, Ytrain, cv=5)\n",
    "result_gboost = cv_score_undersample(\n",
    "    grad_boosting, Xtr_bin.values, Ytrain, cv=5)\n",
    "result_nb = cv_score_undersample(naive_bayes, Xtr_bin.values, Ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary Encoding - Undersampling</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.647071</td>\n",
       "      <td>0.620837</td>\n",
       "      <td>0.340553</td>\n",
       "      <td>0.439838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.681158</td>\n",
       "      <td>0.635291</td>\n",
       "      <td>0.373948</td>\n",
       "      <td>0.470741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.628064</td>\n",
       "      <td>0.373948</td>\n",
       "      <td>0.468630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.716630</td>\n",
       "      <td>0.690158</td>\n",
       "      <td>0.418295</td>\n",
       "      <td>0.520882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.686125</td>\n",
       "      <td>0.662930</td>\n",
       "      <td>0.382711</td>\n",
       "      <td>0.485272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.719132</td>\n",
       "      <td>0.679721</td>\n",
       "      <td>0.420178</td>\n",
       "      <td>0.519312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Accuracy  Precision    Recall        F1\n",
       "Binary Encoding - Undersampling                                         \n",
       "Naive Bayes                      0.647071   0.620837  0.340553  0.439838\n",
       "Logistic Regression              0.681158   0.635291  0.373948  0.470741\n",
       "Linear SVM                       0.682150   0.628064  0.373948  0.468630\n",
       "Random Forest                    0.716630   0.690158  0.418295  0.520882\n",
       "Ada Boost                        0.686125   0.662930  0.382711  0.485272\n",
       "Gradient Boosting                0.719132   0.679721  0.420178  0.519312"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bin = pd.DataFrame(\n",
    "    [result_nb, result_lg, result_lsvm, result_rf, result_aboost, result_gboost])\n",
    "results_bin.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results_bin.rename(index={0: \"Naive Bayes\", 1: \"Logistic Regression\",\n",
    "                          2: \"Linear SVM\", 3: \"Random Forest\",\n",
    "                          4: \"Ada Boost\", 5: \"Gradient Boosting\"}, inplace=True)\n",
    "results_bin.index.name = \"Binary Encoding - Undersampling\"\n",
    "results_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Encoding\n",
    "We now try udnersampling with the leave-one-out encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lg = cv_score_LOO_undersample(\n",
    "    logistic_reg, Xtrain, Ytrain, colnames=col_names, cv=5, scale=True)\n",
    "result_lsvm = cv_score_LOO_undersample(linear_svm, Xtrain, Ytrain,\n",
    "                                       cv=5, colnames=col_names, scale=True)\n",
    "result_rf = cv_score_LOO_undersample(random_forest, Xtrain, Ytrain,\n",
    "                                     colnames=col_names, cv=5)\n",
    "result_aboost = cv_score_LOO_undersample(\n",
    "    ada_boost, Xtrain, Ytrain, colnames=col_names, cv=5)\n",
    "result_gboost = cv_score_LOO_undersample(\n",
    "    grad_boosting, Xtrain, Ytrain, colnames=col_names, cv=5)\n",
    "result_nb = cv_score_LOO_undersample(\n",
    "    naive_bayes, Xtrain, Ytrain, colnames=col_names, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leave-One-Out Encoding - Undersampling</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.581123</td>\n",
       "      <td>0.515828</td>\n",
       "      <td>0.270280</td>\n",
       "      <td>0.354703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.724213</td>\n",
       "      <td>0.682684</td>\n",
       "      <td>0.426769</td>\n",
       "      <td>0.524932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>0.667424</td>\n",
       "      <td>0.435784</td>\n",
       "      <td>0.527131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.726102</td>\n",
       "      <td>0.686289</td>\n",
       "      <td>0.428996</td>\n",
       "      <td>0.527960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.734368</td>\n",
       "      <td>0.683738</td>\n",
       "      <td>0.438968</td>\n",
       "      <td>0.534666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.741069</td>\n",
       "      <td>0.685153</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.541520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Accuracy  Precision    Recall  \\\n",
       "Leave-One-Out Encoding - Undersampling                                  \n",
       "Naive Bayes                             0.581123   0.515828  0.270280   \n",
       "Logistic Regression                     0.724213   0.682684  0.426769   \n",
       "Linear SVM                              0.732649   0.667424  0.435784   \n",
       "Random Forest                           0.726102   0.686289  0.428996   \n",
       "Ada Boost                               0.734368   0.683738  0.438968   \n",
       "Gradient Boosting                       0.741069   0.685153  0.447673   \n",
       "\n",
       "                                              F1  \n",
       "Leave-One-Out Encoding - Undersampling            \n",
       "Naive Bayes                             0.354703  \n",
       "Logistic Regression                     0.524932  \n",
       "Linear SVM                              0.527131  \n",
       "Random Forest                           0.527960  \n",
       "Ada Boost                               0.534666  \n",
       "Gradient Boosting                       0.541520  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_loo = pd.DataFrame(\n",
    "    [result_nb, result_lg, result_lsvm, result_rf, result_aboost, result_gboost])\n",
    "results_loo.columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "results_loo.rename(index={0: \"Naive Bayes\", 1: \"Logistic Regression\",\n",
    "                          2: \"Linear SVM\", 3: \"Random Forest\",\n",
    "                          4: \"Ada Boost\", 5: \"Gradient Boosting\"}, inplace=True)\n",
    "results_loo.index.name = \"Leave-One-Out Encoding - Undersampling\"\n",
    "results_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We noticed that with undersampling the accuracy and recall scores decreased in general for all models and types of encoding, but the precision and F1 scores got better. We noticed that most of the models performed better with the one-hot and leave-one-out encodings than with the binary encodings. We also noticed that most of the models have very similar performance. This might suggest that the data might not have enough information that can help us in detecting crashes with Injury.\n",
    "\n",
    "For further tuning, we choose the algorithms with highest F1-scores: gradient boosting algorithm with leave-one-out encoding and balanced random forest tree with one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fine-tune the two chosen model to see if we can have a boost in the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5483618093786418 600 4\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [300, 500, 600]\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "max_score = 0\n",
    "for n in n_estimators:\n",
    "    for d in max_depth:\n",
    "        clf = GradientBoostingClassifier(n_estimators=n, max_depth=d)\n",
    "        result = cv_score_LOO_undersample(\n",
    "            clf, Xtrain, Ytrain, colnames=col_names, cv=5)\n",
    "        if (result[3] > max_score):\n",
    "            max_score = result[3]\n",
    "            max_n = n\n",
    "            max_d = d\n",
    "print(max_score, max_n, max_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5459190328442887 150\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [50, 100, 150]\n",
    "\n",
    "max_score = 0\n",
    "for n in n_estimators:\n",
    "    clf = BalancedRandomForestClassifier(n_estimators=n)\n",
    "    result = cv_score(clf, Xtr_one_hot.values, Ytrain, cv=5)\n",
    "    if (result[3] > max_score):\n",
    "        max_score = result[3]\n",
    "        max_n = n\n",
    "print(max_score, max_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the performance of the two chosen models, we end up with choosing the gradient boosting classifier (600 for n_estimators, and 4 for max_depth), with the leave-one-out scheme for encoding the categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Selected Model \n",
    "\n",
    "We now train the chosen model with its parameters on the full training set and then test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7469983981659735\n",
      "Precision: 0.6966476415403792\n",
      "Recall: 0.4574985998018181\n",
      "F1: 0.5522962500650128\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier and the under-sampler\n",
    "clf = GradientBoostingClassifier(n_estimators=600, max_depth=4)\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "# Undersample the data\n",
    "x_train, y_train = rus.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# Encode the data using leave-one-out Encoder\n",
    "encoder = ce.leave_one_out.LeaveOneOutEncoder(\n",
    "    cols=col_names).fit(x_train, y_train)\n",
    "x_trans = encoder.transform(x_train)\n",
    "x_test = encoder.transform(Xtest)\n",
    "# Fit the model\n",
    "clf.fit(x_trans, y_train)\n",
    "# Test the model on the test set\n",
    "ypred = clf.predict(x_test)\n",
    "print('Accuracy:', accuracy(ypred, Ytest))\n",
    "print(\"Precision:\", precision(ypred, Ytest))\n",
    "print(\"Recall:\", recall(ypred, Ytest))\n",
    "print(\"F1:\", f1(ypred, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model performed as expected with 74.6% for accuracy and 0.55 for F1-score. \n",
    "\n",
    "Better performance can be achieved by including more information about the crashes, especially those related to the driver. Although the city of Chicago has online information about the vehicle and people involved in the crashes, but those data are mostly empty. \n",
    "\n",
    "There is one last issue that we would like to address; one of the important variable in determining the crash severity is the feature: primary contributory cause. We can check its importance by checking the feature importance returned by the gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features' Importance\n",
      "\n",
      "POSTED_SPEED_LIMIT 0.04119838996558089\n",
      "TRAFFIC_CONTROL_DEVICE 0.015797754325966862\n",
      "DEVICE_CONDITION 0.005238857016511208\n",
      "WEATHER_CONDITION 0.005892923958717377\n",
      "LIGHTING_CONDITION 0.03309817459354405\n",
      "FIRST_CRASH_TYPE 0.4179544513315709\n",
      "TRAFFICWAY_TYPE 0.0706627047691068\n",
      "ALIGNMENT 0.006248160551500281\n",
      "ROADWAY_SURFACE_COND 0.005795432123793122\n",
      "PRIM_CONTRIBUTORY_CAUSE 0.2866866011158217\n",
      "CRASH_HOUR 0.042036840553199076\n",
      "CRASH_DAY_OF_WEEK 0.0066533316136133235\n",
      "CRASH_MONTH 0.008567669160784499\n",
      "AREAS 0.05416870892028976\n"
     ]
    }
   ],
   "source": [
    "feat_imp = clf.feature_importances_\n",
    "print(\"Features' Importance\\n\")\n",
    "for col, importance in zip(X.columns[1:], feat_imp):\n",
    "    print(col, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we obtained with the Cramer'V coefficient, the two most important features are: \"first_crash_type\" (i.e., type of collision) and \"prim_contributory_cause\". Note that the latter feature (\"prim_contributory_cause\") has around 41% of its entries as \"unable to determine\" or \"not applicable\", which we primarily decided to keep them. (All entries of the feature \"first_crash_type\" are known)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of undetermined entries for the primary cause variable is: 41.50697171341908\n"
     ]
    }
   ],
   "source": [
    "unknown = (X.PRIM_CONTRIBUTORY_CAUSE.isin(\n",
    "    ['UNABLE TO DETERMINE', 'NOT APPLICABLE']))\n",
    "print(\"The percentage of undetermined entries for the primary cause variable is:\",\n",
    "      100*sum(unknown)/X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to look at the performance of the predictive model if we only focus on the known entries of the variable \"Primary Contributory Cause\". This is what we are going to check next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on the Known Primary Contributory Cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first discard the unknown causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown = (X.PRIM_CONTRIBUTORY_CAUSE.isin(\n",
    "    ['UNABLE TO DETERMINE', 'NOT APPLICABLE']))\n",
    "X = X.drop(index=X.index[unknown])\n",
    "Y = Y[~unknown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "    X.iloc[:, 1:], Y, train_size=0.8, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the final model we have chosen: gradient boosting classifier and fine-tune it on this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584158578921545 600 4\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [300, 500, 600]\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "max_score = 0\n",
    "for n in n_estimators:\n",
    "    for d in max_depth:\n",
    "        clf = GradientBoostingClassifier(n_estimators=n, max_depth=d)\n",
    "        result = cv_score_LOO_undersample(\n",
    "            clf, Xtrain, Ytrain, colnames=col_names, cv=5)\n",
    "        if (result[3] > max_score):\n",
    "            max_score = result[3]\n",
    "            max_n = n\n",
    "            max_d = d\n",
    "print(max_score, max_n, max_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now check its final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7331356932894505\n",
      "Precision: 0.701985508610144\n",
      "Recall: 0.500167616493463\n",
      "F1: 0.5841359329731423\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier and the under-sampler\n",
    "clf = GradientBoostingClassifier(n_estimators=600, max_depth=4)\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "# Undersample the data\n",
    "x_train, y_train = rus.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# Encode the data using leave-one-out Encoder\n",
    "encoder = ce.leave_one_out.LeaveOneOutEncoder(\n",
    "    cols=col_names).fit(x_train, y_train)\n",
    "x_trans = encoder.transform(x_train)\n",
    "x_test = encoder.transform(Xtest)\n",
    "# Fit the model\n",
    "clf.fit(x_trans, y_train)\n",
    "# Test the model on the test set\n",
    "ypred = clf.predict(x_test)\n",
    "print('Accuracy:', accuracy(ypred, Ytest))\n",
    "print(\"Precision:\", precision(ypred, Ytest))\n",
    "print(\"Recall:\", recall(ypred, Ytest))\n",
    "print(\"F1:\", f1(ypred, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we have a boost in the performance in terms of recall and F1-score, when we only focus on the known entries of the feature \"prim_contributory_cause\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that better performance can be achieved by including more information about the crashes, especially those related to the driver. Although the city of Chicago has online information about the vehicle and people involved in the crashes, but those data are mostly empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
