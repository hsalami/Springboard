{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This report describes the steps done to clean and combine the datasets of the second capstone project. The aim of the project is to predict the violations of restaurants in Allegheny county.\n",
    "\n",
    "The datasets come from two sources:\n",
    "\n",
    "- Allegheny county inspection dataset, which includes the types of violations found during restaurants' inspections;\n",
    "- Yelp datasets, which include information and text reviews of the restaurants.\n",
    "\n",
    "In this project, we want to collect for each restaurant the information and reviews that preceded its inspection. The first source includes information about each inspection and the second source has the reviews. \n",
    "\n",
    "In this first part of the project, we examine and clean the inspection dataset from Allegheny county source and the business dataset (which only includes restaurants information without the reviews) from Yelp source. We will then find the common restaurants mentioned in both datasets, in order to combine the information of the restaurants provided by the two sources.\n",
    "\n",
    "This report is divided as follows:\n",
    "- we first load and describe each dataset that we refer as violation and Yelp business datasets;\n",
    "- we then examine the violation dataset by filling some missing entries, processing the names of facilities and counting the types of violation for each inspection;\n",
    "- we also examine the Yelp business dataset and process the names of the restaurants;\n",
    "- we finally find the common restaurants mentioned in both datasets by comparing their names and addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Violation and Yelp Business datasets\n",
    "\n",
    "We here load and describe the two datasets that we will be working with in this first part of the project.\n",
    "\n",
    "### Violation Dataset\n",
    "\n",
    "The county of Allegheny keeps a record of each inspection done and the violation found in each inspection. The dataset is available from the website of the county. Let us examine the dataset and check its entries.\n",
    "\n",
    "We first load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "violations = pd.read_csv(\"violations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['encounter', 'id', 'placard_st', 'facility_name', 'bus_st_date',\n",
       "       'description', 'description_new', 'num', 'street', 'city', 'state',\n",
       "       'zip', 'inspect_dt', 'start_time', 'end_time', 'municipal', 'rating',\n",
       "       'low', 'medium', 'high', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of this dataset indicates the violation type found during an inspection of a restaurant in Allegheny county. The \"encounter\" column contains the identification number of each inspection, \"inspect_dt\", \"start_time\" and \"end_time\" columns represent respectively the date and time of the inspection. The column \"description_new\" contains a description of the violation found and the columns \"low\", \"medium\" and \"high\" indicate the type of the violation. The information of the restaurant inspected is provided by the columns: facility name, business starting date ('bus_st_date'), restaurant category (\"description\"), and address (\"num\", \"street\", \"city\", \"state\", \"zip\", \"municipal\"). Also each restaurant is identified by an identification number (ID) indicated by the column \"id\". Note that multiple rows could belong to the same inspection of a given restaurant, since each row belongs to one violation and multiple violations could be found at the same restaurant during an inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Business Dataset\n",
    "\n",
    "Two of the files provided by Yelp datasets are business and review files (json files). The business file includes information about the restaurants and associates each restaurant with an identification number. The review file contain Yelp text reviews for each restaurant. We here focus on the business dataset, and map the restaurants of the violation dataset to the restaurants in the Yelp dataset.\n",
    "\n",
    "We load the business json file and extract only the restaurants of Pennsylvania state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "business = [json.loads(line)\n",
    "            for line in open('YELP/business.json', encoding=\"utf8\")]\n",
    "bus_df = pd.DataFrame(business)\n",
    "bus_df = bus_df[bus_df.state == \"PA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YELP business dataset consists of the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
       "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
       "       'attributes', 'categories', 'hours'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains the name, address, attributes and categories of each restaurant, as well as the number of stars and reviews given to each restaurant. Moreover, each restaurant of Yelp dataset is associated with a business identification number (business ID ). Our target is that after we process the violation and business datasets, we want to map the ID of the restaurants in the violation dataset to the business ID of the restaurants in the Yelp dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of the Violation Dataset\n",
    "\n",
    "We now focus on cleaning the violation dataset. More specifically, we fill the missing addresses, group the data by inspection and count the number of each type of violations found in each inspection, and finally fix some of the inconsistencies in the names and IDs of the facilities inspected. \n",
    "\n",
    "### Filling the Missing Entries\n",
    "\n",
    "The data contains some missing entries in the columns related to restaurants' information, namely: facility name, business starting date, description and addresses. The county website has a dataset for the facilities inspected that we can use here to fill the missing entries. More precisely, we extract from the dataset of facilities the missing information using the ID of the restaurants with missing entries.\n",
    "\n",
    "We load the dataset of facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities = pd.read_csv(\"facilities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the missing entries of the corresponding columns using the facilities dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fill = [\"facility_name\", \"bus_st_date\", 'description',\n",
    "                'num', 'street', 'city', 'state', 'zip', 'municipal']\n",
    "\n",
    "for col_name in cols_to_fill:\n",
    "    \n",
    "    # get the IDs of facilities with empty entries in the column col_name\n",
    "    ids_none = violations[violations[col_name].isnull()].id\n",
    "    \n",
    "    # get the info from faciltites dataset\n",
    "    entries_none = facilities[(facilities.id.isin(list(ids_none)))][[\n",
    "        \"id\", col_name]]\n",
    "    entries = entries_none.set_index('id').T.to_dict(orient='list')\n",
    "    \n",
    "    # fill the missing entries\n",
    "    for ind in ids_none.index:\n",
    "        try:\n",
    "            violations.at[ind, col_name] = entries[violations.at[ind, 'id']][0]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the facilities dataset, the columns of \"facility name\", \"description\" and \"street\" are completely filled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(violations.description.isnull()))\n",
    "print(sum(violations.facility_name.isnull()))\n",
    "print(sum(violations.street.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the column (\"num\"), which represents the number of street, still have some missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443\n"
     ]
    }
   ],
   "source": [
    "print(sum(violations.num.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the name of the streets and the zip code of the restaurants with empty entries for the number of street ('num'), we notice that most of them correspond to restaurants in Pittsburgh airport or in a mall, so that the airport or the name of the mall is given as the address in the column 'street' instead of the real address, and the column 'num' is left empty. We next focus on filling some of these missing street numbers, and we leave the remaining entries to be filled after we map the restaurants of both datasets.\n",
    "\n",
    "We first look at the zip codes of the restaurants with empty street number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15231.0    973\n",
       "15213.0    100\n",
       "15232.0     74\n",
       "15236.0     36\n",
       "15238.0     36\n",
       "15146.0     35\n",
       "15026.0     29\n",
       "15056.0     27\n",
       "15116.0     25\n",
       "15102.0     19\n",
       "15084.0     18\n",
       "15223.0     14\n",
       "15065.0     12\n",
       "15045.0      9\n",
       "15222.0      8\n",
       "15104.0      7\n",
       "15241.0      6\n",
       "15240.0      4\n",
       "15215.0      4\n",
       "15147.0      3\n",
       "15205.0      2\n",
       "15108.0      1\n",
       "15219.0      1\n",
       "Name: zip, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations[violations.num.isnull()].zip.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that most of the entries correspond to the zip code: 15231. Let us check the street names of the corresponding zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pgh Intl Airport / AC-17        55\n",
       "Pgh Intl Airport / AC-2B        53\n",
       "Pgh Intl Airport / AC-2D        29\n",
       "Pgh Intl Airport / SE-1         28\n",
       "Pgh Int'l Airport / AC-20A/B    28\n",
       "                                ..\n",
       "Pgh Intl Airport / NE-13         1\n",
       "Pgh Int'l Airport / AC-14        1\n",
       "Pgh Intl Airport / NE-9A         1\n",
       "Pgh Intl Airport /  AC-1A        1\n",
       "Pgh Intl Airport / AC-32A        1\n",
       "Name: street, Length: 77, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_to_check = np.logical_and(\n",
    "    violations.num.isnull(), violations.zip == 15231)\n",
    "violations[where_to_check].street.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these restaurants are in Pittsburgh airport. In order to make the mapping of restaurants information easier between this dataset and Yelp dataset, we will modify the address of the restaurants that are in Pittsburgh airport to : 1000 Airport Blvd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.at[where_to_check,'num'] = 1000\n",
    "violations.at[where_to_check,'street'] = \"Airport Blvd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining restaurants with empty \"num\", we manually fill some of the missing entries by focusing on the mostly empty entries and on the restaurants that are in malls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "India on Wheels (YXN-3894) MFF4    63\n",
       "Chakh Le India (YBV-7676) MFF4     37\n",
       "Name: facility_name, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_to_check = np.logical_and(\n",
    "    violations.num.isnull(), violations.zip == 15213)\n",
    "violations[where_to_check].facility_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.at[violations.facility_name ==\n",
    "              \"India on Wheels (YXN-3894) MFF4\", \"num\"] = 4422\n",
    "violations.at[violations.facility_name ==\n",
    "              \"India on Wheels (YXN-3894) MFF4\", \"street\"] = \"Bigelow\"\n",
    "violations.at[violations.facility_name ==\n",
    "              \"Chakh Le India (YBV-7676) MFF4\", \"num\"] = 4341\n",
    "violations.at[violations.facility_name ==\n",
    "              \"Chakh Le India (YBV-7676) MFF4\", \"street\"] = \"Bigelow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Little Caesars Pizza #145    32\n",
       "GNC #5306                     4\n",
       "Name: facility_name, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_to_check = np.logical_and(violations.num.isnull(), violations.zip ==15236)\n",
    "violations[where_to_check].facility_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.at[violations.facility_name==\"Little Caesars Pizza #145\",\"num\"] =5301 \n",
    "violations.at[violations.facility_name==\"Little Caesars Pizza #145\",\"street\"]=\"Grove Rd\"\n",
    "violations.at[violations.facility_name==\"GNC #5306\",\"num\"] =5301 \n",
    "violations.at[violations.facility_name==\"GNC #5306\",\"street\"]=\"Grove Rd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gloria Jean's Coffees            24\n",
       "Foxwood Park Forest Swim Club     7\n",
       "Auntie Anne's Pretzels #PA287     4\n",
       "Name: facility_name, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_to_check = np.logical_and(violations.num.isnull(), violations.zip ==15146)\n",
    "violations[where_to_check].facility_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.at[violations.facility_name == \"Gloria Jean's Coffees\", \"num\"] = 200\n",
    "violations.at[violations.facility_name ==\n",
    "              \"Gloria Jean's Coffees\", \"street\"] = \"Mall Blvd\"\n",
    "violations.at[violations.facility_name ==\n",
    "              \"Auntie Anne's Pretzels #PA287\", \"num\"] = 145\n",
    "violations.at[violations.facility_name ==\n",
    "              \"Auntie Anne's Pretzels #PA287\", \"street\"] = \"Mall Blvd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Persin's Tavern    29\n",
       "Name: facility_name, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_to_check = np.logical_and(\n",
    "    violations.num.isnull(), violations.zip == 15026)\n",
    "violations[where_to_check].facility_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.at[violations.facility_name == \"Persin's Tavern\", \"num\"] = 1286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Giant Eagle Cafe #37                 17\n",
       "GNC Distribution Center Warehouse     3\n",
       "GNC #3710                             2\n",
       "Henle Park Concession Stand           2\n",
       "GetGo #3137                           2\n",
       "AFC Sushi @ Giant Eagle #37           1\n",
       "Name: facility_name, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_to_check = np.logical_and(\n",
    "    violations.num.isnull(), violations.zip == 15056)\n",
    "violations[where_to_check].facility_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Giant Eagle Cafe #37\", \"GetGo #3137\",\n",
    "         \"GNC #3710\", \"AFC Sushi @ Giant Eagle #37\"]\n",
    "for name in names:\n",
    "    violations.at[violations.facility_name == name,\n",
    "                  \"street\"] = \"Quaker Village Shopping Ctr\"\n",
    "    violations.at[violations.facility_name == name, \"num\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filled most of the missing \"num\" entries, but there are still some missing entries, which we will leave for now empty and fill them when we combine the violations dataset with the yelp business dataset. We also have some missing entries in the column \"bus_st_date\" (business starting date), which we will also leave empty until we combine the datasets, so that we focus only on the common restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(violations.bus_st_date.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.at[violations.bus_st_date.isnull(), 'bus_st_date'] = \"none\"\n",
    "violations.at[violations.num.isnull(), 'num'] = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally remove some redundant rows where facility name is given as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_to_drop = violations[(violations.facility_name == \"test\")].index\n",
    "violations = violations.drop(index=where_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting each Type of Violation per Inspection\n",
    "\n",
    "Each row of the dataset represents one type of violation detected during an inspection. Since multiple types of violation could be detected during an inspection, multiple rows in the dataset can correspond to the same inspection. We now transform this dataset into a new one, where each row represents one inspection and includes the number of each type of violation detected.\n",
    "\n",
    "We first divide the columns into two sets: one that contains the information of the restaurants inspected and another set that contains the type of each violation found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['encounter', 'id', 'placard_st', 'facility_name', 'bus_st_date',\n",
    "         'description', 'num', 'street', 'city', 'state',\n",
    "         'zip', 'inspect_dt', 'municipal']\n",
    "\n",
    "cols2 = ['encounter', 'low', 'medium', 'high']\n",
    "\n",
    "viol1 = violations[cols1]\n",
    "viol2 = violations[cols2]\n",
    "\n",
    "viol1 = viol1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"low\", \"medium\" and \"high\" have true or false values. We map those values to 1 or 0 respectively, before we add them for each inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol2 = viol2.fillna(0)\n",
    "viol2 = viol2.replace({'F': 0, 'T': 1})\n",
    "viol2 = viol2.groupby(['encounter'])[['low', 'medium', 'high']].sum()\n",
    "viol2 = viol2.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally merge the two sets to obtain a new dataset, where each row corresponds to one inspection and the number of each type of violations found during the inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation = viol1.merge(viol2, on='encounter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55416, 16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sequel, we only focus on this new violation dataset where each row represents one inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Names of facilities\n",
    "\n",
    "While exploring the violation data, we noticed that some restaurants have multiple ID numbers. This can be seen by computing the number of unique values of ID and facility names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10299\n",
      "11325\n"
     ]
    }
   ],
   "source": [
    "print(len(violation.facility_name.unique()))\n",
    "print(len(violation.id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique IDs is higher than the number of unique facility names. One reason for this discrepancy is that restaurants that have more than one branches can all have the same name (for example chain restaurants). Another reason for this discrepancy, is that the name of the same restaurant can be spelled differently by the inspectors. To address this issue, we next focus on making the names of facilities more consistent and then adjust the IDs of the restaurants that have multiple IDs. \n",
    "\n",
    "We first modify the names of the restaurants by making the letters lowercase, removing the apostrophes and underscores, replacing \"&\" with \"and\" and any double spaces with a single space. The decisions to make these specific modifications was made based on the observation of the facility names. Moreover, this processing step will also be helpful when merging this dataset with Yelp business dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation['facility_name'] = violation['facility_name'].str.lower()\n",
    "violation['facility_name'] = violation['facility_name'].str.replace('&', 'and')\n",
    "violation['facility_name'] = violation['facility_name'].str.replace('\\'', '')\n",
    "violation['facility_name'] = violation['facility_name'].str.replace('-', '')\n",
    "violation[\"facility_name\"] = violation.facility_name.str.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the names of the restaurants, the next step is to check the restaurants that have same name and address but different IDs. To make sure we have consistent addresses, we modify the street names by removing any double spaces and replacing the types of streets with their abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation[\"street\"] = violation.street.str.replace(\"  \",\" \")\n",
    "violation[\"street\"] = violation.street.str.replace(\"Road\",\"Rd\")\n",
    "violation[\"street\"] = violation.street.str.replace(\"Avenue\",\"Ave\")\n",
    "violation[\"street\"] = violation.street.str.replace(\"Street\",\"St\")\n",
    "violation[\"street\"] = violation.street.str.replace(\"Boulevard\",\"Blvd\")\n",
    "violation[\"street\"] = violation.street.str.replace(\"William\",\"Wm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the street name from the \"street\" column by dropping the direction symbols (\"N\", \"E\", \"W\", \"S\") if present. This is done because a same restaurant might be given two addresses, where the only difference is the presence or absence of the direction symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function extracts the street name\n",
    "def extract_street(street_name):\n",
    "    # split the strings\n",
    "    streets = street_name.split(' ')\n",
    "    if (len(streets) > 1):\n",
    "        # if the street name starts with\n",
    "        # a direction's character\n",
    "        # extract the next string\n",
    "        if (len(streets[0]) == 1):\n",
    "            return streets[1]\n",
    "    return streets[0]\n",
    "\n",
    "# make a column \"st\" for the street name\n",
    "violation['st'] = violation[\"street\"].apply(extract_street)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check the facilities that have the same name and address but with different IDs. After we find these facilities, we adjust their IDs and business starting dates, so that they only have one ID and one starting date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the violation by facility name, street nnumber and street name\n",
    "# compute for each group the number of unique IDs\n",
    "fnames = violation.groupby(['facility_name', 'num', 'st'])['id'].nunique()\n",
    "# extract restaurants with more than one ID\n",
    "fnames = fnames[fnames > 1]\n",
    "fnames = fnames.reset_index().drop(columns='id')\n",
    "\n",
    "# find the minimum ID for each group\n",
    "fnames_id = violation.groupby(['facility_name', 'num', 'st'])['id'].min()\n",
    "# find the minimum business starting date for each group\n",
    "fnames_date = violation.groupby(['facility_name', 'num', 'st'])[\n",
    "    'bus_st_date'].min()\n",
    "\n",
    "# assign the minimum ID and business starting dates for \n",
    "# restaurants with more than one ID\n",
    "fnames_tot = (fnames.merge(fnames_id.reset_index())\n",
    "              ).merge(fnames_date.reset_index())\n",
    "fnames_tot = fnames_tot.rename(\n",
    "    columns={'id': 'id_2', 'bus_st_date': 'bus_st_date_2'})\n",
    "violation = violation.merge(fnames_tot, how='left')\n",
    "for index, row in violation.iterrows():\n",
    "    if (~np.isnan(row['id_2'])):\n",
    "        violation.at[index, 'id'] = row['id_2']\n",
    "        violation.at[index, 'bus_st_date'] = row['bus_st_date_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation = violation.drop(columns=['bus_st_date_2', 'id_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a same restaurant with different IDs, we looked at the names, street numbers and street names. However, we also noticed from our observations that for same restaurant, different street numbers are entered (for instance instead of 424 Beaver st, we have 428 Beaver St.) This is another reason why some restaurants have multiple IDs. We next find the restaurants with same name, same street number, same zip code but with different IDs, and then check their street numbers. If their street numbers are close, we adjust the IDS and the business starting dates of the restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a function that extracts the first portion of the street number, this is because some street number are followed by a letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function extracts the number street\n",
    "# in case the number is followed by a letter\n",
    "\n",
    "def extract_num(number):\n",
    "    nums = str(number).split(' ')\n",
    "    return nums[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the restaurant by their names, street name and zip code. We then extract those with many IDs, check their street numbers and focus only on those with close street numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the street number\n",
    "violation[\"num_st\"] = violation.num.apply(extract_num)\n",
    "\n",
    "# group the violation by facility name, street name and zip code\n",
    "# compute for each group the number of unique IDs\n",
    "fnames = violation.groupby(['facility_name', 'st', 'zip'])['id'].nunique()\n",
    "# extract restaurants with more than one ID\n",
    "fnames = fnames[fnames > 1]\n",
    "fnames = fnames.reset_index().drop(columns='id')\n",
    "\n",
    "# find the minimum ID for each group\n",
    "fnames_id = violation.groupby(['facility_name', 'st', 'zip'])['id'].min()\n",
    "# find the minimum business starting date for each group\n",
    "fnames_date = violation.groupby(['facility_name', 'st', 'zip'])[\n",
    "    'bus_st_date'].min()\n",
    "# find the maximum street number\n",
    "fnames_num_max = violation.groupby(['facility_name', 'st', 'zip'])[\n",
    "    'num_st'].max()\n",
    "fnames_num_max = fnames_num_max.reset_index().rename(\n",
    "    columns={'num_st': 'num_max'})\n",
    "# find the minimum street number\n",
    "fnames_num_min = violation.groupby(['facility_name', 'st', 'zip'])[\n",
    "    'num_st'].min()\n",
    "fnames_num_min = fnames_num_min.reset_index().rename(\n",
    "    columns={'num_st': 'num_min'})\n",
    "\n",
    "# assign the minimum ID and business starting dates for \n",
    "# restaurants with more than one ID and that have close \n",
    "# street numbers\n",
    "fnames_tot = (fnames.merge(fnames_id.reset_index())\n",
    "              ).merge(fnames_date.reset_index())\n",
    "fnames_tot = (fnames_tot.merge(fnames_num_min)).merge(fnames_num_max)\n",
    "fnames_tot = fnames_tot[fnames_tot.num_max.astype(\n",
    "    int)-fnames_tot.num_min.astype(int) < 1500]\n",
    "fnames_tot = fnames_tot.rename(\n",
    "    columns={'id': 'id_2', 'bus_st_date': 'bus_st_date_2'})\n",
    "\n",
    "violation = violation.merge(fnames_tot, how='left')\n",
    "\n",
    "for index, row in violation.iterrows():\n",
    "    if (~np.isnan(row['id_2'])):\n",
    "        violation.at[index, 'id'] = row['id_2']\n",
    "        violation.at[index, 'bus_st_date'] = row['bus_st_date_2']\n",
    "\n",
    "violation = violation.drop(\n",
    "    columns=['bus_st_date_2', 'id_2', 'num_st', 'num_max', 'num_min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjusted the information of the restaurants by making the names and addresses of the restaurants more consistent. We then identified the restaurants that have many IDs and adjusted their IDs, by primarily focusing on the name of the facilities and their addresses. However, it is also possible that for a same facility, different names were entered. We will address this issue when we extract more information about the facilities using YELP dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of the Yelp Business Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now process the name and address of each restaurant in Yelp business dataset, similarly to what we have early done with the violation dataset. This is in order to make the mappings of the restaurants between the two datasets easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_df['name'] = bus_df['name'].str.lower()\n",
    "bus_df['name'] = bus_df['name'].str.replace('&', 'and')\n",
    "bus_df['name'] = bus_df['name'].str.replace('\\'', '')\n",
    "bus_df['name'] = bus_df['name'].str.replace('-', '')\n",
    "bus_df['name'] = bus_df['name'].str.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_df['address'] = bus_df['address'].str.replace(\"  \", \" \")\n",
    "bus_df['address'] = bus_df['address'].str.replace(\"Road\", \"Rd\")\n",
    "bus_df['address'] = bus_df['address'].str.replace(\"Avenue\", \"Ave\")\n",
    "bus_df['address'] = bus_df['address'].str.replace(\"Street\", \"St\")\n",
    "bus_df['address'] = bus_df['address'].str.replace(\"Boulevard\", \"Blvd\")\n",
    "bus_df['address'] = bus_df['address'].replace(\"William\", \"Wm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the address column, we extract the street name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function extract the street name\n",
    "# from the address\n",
    "def extract_street_y(address):\n",
    "    streets = address.split(' ')\n",
    "    if(len(streets)==1):\n",
    "        return streets[0]\n",
    "    streets = streets[1:]\n",
    "    if (len(streets)>1):\n",
    "        if (len(streets[0])==1):\n",
    "            return streets[1]\n",
    "    return streets[0]\n",
    "\n",
    "bus_df['sty'] = bus_df[\"address\"].apply(extract_street_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the IDs of the Restaurants\n",
    "\n",
    "We now focus on mapping the common restaurants in both datasets. To map the restaurants of the two datasets, we rely on the name and address of the restaurants. We first map the restaurants that have the same name in both datasets. We then map the restaurants where one name is a substring of the other name, this is because the names of some restaurants are inconsistent between the two datasets. \n",
    "\n",
    "We first extract the names and addresses of the restaurants of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the names and addresses from yelp and violation datasets\n",
    "name_add_yelp = bus_df[['business_id', 'name',\n",
    "                        'address', 'city', 'sty', 'postal_code']]\n",
    "name_add_viol = violation[['id', 'facility_name',\n",
    "                           'num', 'street', 'city', 'st', 'zip']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge the information of the restaurants based on the names of the facilities. This is an outer merge, from which we are going next to extract the restaurants with similar names in both datasets and those with different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two sets of info using the names of the restaurants\n",
    "viol_yelp_merged = name_add_viol.merge(\n",
    "    name_add_yelp, left_on=\"facility_name\", right_on=\"name\", how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Restaurants with Same Name in Both Datasets\n",
    "We now find the common restaurants of both datasets with exact naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on restaurants with same name\n",
    "# i.e., extract the non-null entries in the merged dataframe (outer merge)\n",
    "viol_yelp_same = viol_yelp_merged[np.logical_and(\n",
    "    viol_yelp_merged.name.notnull(), viol_yelp_merged.facility_name.notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_same = viol_yelp_same.sort_values(\"facility_name\")\n",
    "viol_yelp_same = viol_yelp_same.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since restaurants with same name can have many branches, we keep the restaurants with same street name and zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the restaurants (with same name) that are on the same street\n",
    "viol_yelp_same = viol_yelp_same[viol_yelp_same[\"st\"] == viol_yelp_same[\"sty\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the restaurants (with same name) that are have the same zipcode\n",
    "viol_yelp_same = viol_yelp_same[viol_yelp_same[\"zip\"].astype(\n",
    "    int) == viol_yelp_same[\"postal_code\"].astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the list of restaurants with the same name and address of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_same.to_csv(\"viol_yelp_same.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Restaurants with Slightly Different Naming\n",
    "\n",
    "We now focus on finding the common restaurants that appear with different names in the two datasets. To do this, we map the restaurants with names that are subsets of each other. We then check their address and keep the restaurants with same address.\n",
    "\n",
    "We first find the restaurants that appear with different names in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on restaurants with same name\n",
    "# i.e., extract the rows with null entries in the merged dataframe\n",
    "viol = viol_yelp_merged[viol_yelp_merged.name.isnull()][[\n",
    "    'id', 'facility_name', 'num', 'street', 'city_x', 'st', 'zip'\n",
    "]].drop_duplicates()\n",
    "\n",
    "yelp = viol_yelp_merged[viol_yelp_merged.facility_name.isnull()][[\n",
    "    'business_id', 'name', 'address', 'city_y', 'sty', 'postal_code'\n",
    "]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then map the restaurants with names that are substring of each others. For each restaurant in the violation dataset, we check the names of the restaurants in the yelp business dataset that start with the same character and that are either a substring or superstring of the restaurant's name of the violation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_diff = []\n",
    "\n",
    "# ascii codes of characters and symbols\n",
    "# the codes of upper case are excluded\n",
    "codes = list(range(32, 65))+list(range(92, 127))\n",
    "\n",
    "for code in codes:\n",
    "    \n",
    "    # get the character\n",
    "    c = chr(code)\n",
    "    \n",
    "    # get the facility names that starts with the character c\n",
    "    # from both datasets\n",
    "    viol_c = viol[viol.facility_name.apply(\n",
    "        lambda s: s.startswith(c))].sort_values('facility_name')\n",
    "    yelp_c = yelp[yelp.name.apply(\n",
    "        lambda s: s.startswith(c))].sort_values('name')\n",
    "    \n",
    "    # combine the restaurant information from both datasets\n",
    "    # when one name is subset of the other name \n",
    "    for indexv, rowv in viol_c.iterrows():\n",
    "        for indexy, rowy in yelp_c.iterrows():\n",
    "            if ((rowy['name'] in rowv['facility_name']) \n",
    "                or (rowv['facility_name'] in rowy['name'])):\n",
    "                rest = list(rowv)+list(rowy)\n",
    "                viol_yelp_diff.append(rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the obtained results into a panda dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_diff = pd.DataFrame(\n",
    "    viol_yelp_diff, columns=list(viol.columns)+list(yelp.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the names of the restaurants that are slightly different between the two datasets, we only keep the restaurants with same addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the restaurants with samme street name and zip codes\n",
    "viol_yelp_diff = viol_yelp_diff[viol_yelp_diff[\"st\"] == viol_yelp_diff[\"sty\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_diff = viol_yelp_diff[viol_yelp_diff[\"zip\"].astype(\n",
    "    int) == viol_yelp_diff[\"postal_code\"].astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally save the obtained mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_diff.to_csv(\"viol_yelp_diff.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Processing \n",
    "\n",
    "After having found the common restaurants in both datasets based on their names and addresses, we manually checked the mappings found. We noticed some more inconsistencies that were not solved by the previous processing steps. We address these inconsistencies here based on our observations: we first remove the incorrect mappings, we then check if any business ID is mapped to more than one ID and any ID is mapped to more than one business ID.\n",
    "\n",
    "### Deleting some Wrong Mappings\n",
    "\n",
    "We now delete some rows from the dataframes of mappings: viol_yelp_same and viol_yelp_diff; these rows contain wrong mappings. For instance, a cafe from a store from the violation dataset is mapped to the whole store mentioned in Yelp business dataset, or chain restaurants that are on the same street but with different street numbers. Note that we explicitly specify the rows to drop based on our manual checking not in an automated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# drop rows with wrong mapping\n",
    "where_to_drop_1 = (viol_yelp_same.name == \"mcdonalds\") & (\n",
    "    viol_yelp_same.num == \"6361\") & (viol_yelp_same.business_id == \"G9LyIc5LgBNM_zF8BJMhNg\")\n",
    "where_to_drop_2 = (viol_yelp_same.name == \"crazy mocha\") & (\n",
    "    viol_yelp_same.num == \"801\") & (viol_yelp_same.business_id == \"87rslhpXfVcJXN1_DiUQ9A\")\n",
    "\n",
    "viol_yelp_same = viol_yelp_same[~where_to_drop_1]\n",
    "viol_yelp_same = viol_yelp_same[~where_to_drop_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facility name, street number and business id of rows to drop\n",
    "# those are based on checking the mappings found eariler\n",
    "\n",
    "rows_to_drop = [[\"agh suburban campus gift shop\", \"100\", \"zPaoppBtXodfjEnsT7GFZA\"],\n",
    "                [\"agh suburban campus kitchen\", \"100\", \"zPaoppBtXodfjEnsT7GFZA\"],\n",
    "                [\"aldi #69\", \"8000\", \"ZOAbx2hTdu8KyUMDicZNDw\"],\n",
    "                [\"au bon pain #103 @ us steel tower concourse\",\n",
    "                    \"600\", \"mNMVqHsgJq6YErFPfaUlig\"],\n",
    "                [\"au bon pain #103 @ us steel tower concourse\",\n",
    "                    \"600\", \"9toyHY_tXx4eenp8Oxmmsg\"],\n",
    "                [\"au bon pain #224 @ gulf tower\", \"707\", \"9toyHY_tXx4eenp8Oxmmsg\"],\n",
    "                [\"au bon pain @ one oxford center plaza level\",\n",
    "                    \"301\", \"mNMVqHsgJq6YErFPfaUlig\"],\n",
    "                [\"casa rastalas palmas\", \"2056\", \"iXnHrmTw-r6LNzC2MjjrWQ\"],\n",
    "                [\"chipotle mexican grill #2410\", \"4611\", \"VFv7NcPW9ajUTLleJ8wOQA\"],\n",
    "                [\"chipotle mexican grill #863\", \"3619\", \"o7aBPJhTR-R4TccTSfu-Ng\"],\n",
    "                [\"eatn park #3\", \"7370\", \"6fQXFkkw2BUARDa0EzsGnA\"],\n",
    "                [\"eatn park restaurants #80\", \"516\", \"cTYEiHz8AEOpiwzDY9ngWQ\"],\n",
    "                [\"getgo #3047\", \"6513\", \"x_9malt5q6yHZwYdX6k6Hg\"],\n",
    "                [\"getgo #3057\", \"6513\", \"x_9malt5q6yHZwYdX6k6Hg\"],\n",
    "                [\"getgo fuel kiosk #3257 robinson\",\n",
    "                    \"6513\", \"x_9malt5q6yHZwYdX6k6Hg\"],\n",
    "                [\"giant eagle #1691\", \"400\", \"bNkCDwXWscwnVV3xixQoAg\"],\n",
    "                [\"giant eagle #24\", \"3239\", \"PJcOOjebn86geLXG2qPB_Q\"],\n",
    "                [\"giant eagle #63\", \"4250\", \"k8NHw2fUjisdp38PLCl0JA\"],\n",
    "                [\"giant eagle #641\", \"1025\", \"6FqYbhI4bClySEVbdHnOaQ\"],\n",
    "                ['giant eagle #67', '8080', 'Ea-6xpe581a_mmeD3W3mog'],\n",
    "                ['ikea restaurant', '2001', 'ohkd4oHpIrvue5nWBPMSMg'],\n",
    "                ['pan', '3519', '6MQJMPVi5HobGjx73DaE7Q'],\n",
    "                ['panera bread #4329', '117', 'fM9Nmx3Rv4zFU5fLMILpLA'],\n",
    "                ['starbucks coffee #19816 mcknight',\n",
    "                    '7707', 'y90JVPFQ_TWGQaBhHtSm3w'],\n",
    "                ['starbucks coffee #21587', '301', 'UQtV1plcxfLTlvKeLPv63Q'],\n",
    "                ['starbucks coffee #22032', '3007', 'NeM7anGnTOTn7sEJavS3sw'],\n",
    "                ['starbucks coffee #760', '5211', 'Vgh-CjwAl4tFleP39OdzaA'],\n",
    "                ['starbucks coffee #7625', '5310', 'SY7ZyPxidnToTEFbJBleOg'],\n",
    "                ['starbucks coffee #7749', '4765', 'Hi9Mq6SkJRU9E0YzPAl7PQ'],\n",
    "                ['starbucks coffee #776', '4885', 'A-OvzL1cssAoFQ-TMRUpBQ'],\n",
    "                ['starbucks coffee #7875', '1597', 'qnN6BumIj-OPU38Bg6wPOw'],\n",
    "                ['starbucks coffee / baggage claim',\n",
    "                    1000, 'TrZVtAQivYyGDOQH46_aFA'],\n",
    "                ['wendys old fashioned hamburgers #1622367',\n",
    "                    '891', 'E4U8RCe42CpT3rtof7hEwQ'],\n",
    "                ['wendys old fashioned hamburgers #1632356',\n",
    "                    '2691', 'GaP5KfdlDuNasKgTNR8Saw'],\n",
    "                ['wendys old fashioned hamburgers #528',\n",
    "                    '891', 'E4U8RCe42CpT3rtof7hEwQ'],\n",
    "                ['wendys old fashioned hamburgers #529',\n",
    "                    '2691', 'GaP5KfdlDuNasKgTNR8Saw'],\n",
    "                ['starbucks coffee #75819', '427', 'A89Re1NzGXBNuAk6CZ1rwQ'],\n",
    "                ['rite aid phamacies #10931', '2501', 'DDRXDnU_BBbGg9U2RYWiAQ'],\n",
    "                ['eatn park #13', '7671', 'MjOFnVxoKeOikgBGTZ-UcA'],\n",
    "                ['rite aid pharmacies #10906', '5235', 'yl97wnawcJ14L_o5xaQ7bA']]\n",
    "\n",
    "for row in rows_to_drop:\n",
    "    where_to_drop = (viol_yelp_diff.facility_name == row[0]) & (\n",
    "        viol_yelp_diff.num == row[1]) & (viol_yelp_diff.business_id == row[2])\n",
    "    viol_yelp_diff.drop(\n",
    "        index=viol_yelp_diff[where_to_drop].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the names of restaurants whose rows are to drop\n",
    "names = [\"carlow university\", \"chartiers country club\", \"chatham university\",\n",
    "         \"heinz field\", \"gandy dancer saloon\", \"grand concourse\", \n",
    "         \"hyatt place pittsburgh airport\", \"hyatt place pittsburgh airport\",\n",
    "         \"pittsburgh airport marriott\", \"pittsburgh field club\",\n",
    "         \"riverhounds\", \"rivers casino\", \"target\", \"upmc mercy\", \n",
    "         \"upmc\", \"upmc presbyterian\"]\n",
    "\n",
    "for name in names:\n",
    "    where_to_drop = (viol_yelp_diff.name == name)\n",
    "    viol_yelp_diff.drop(\n",
    "        index=viol_yelp_diff[where_to_drop].index, inplace=True)\n",
    "\n",
    "# the facility names of restaurants whose rows are to drop\n",
    "fa_names = [\"microtel inn and suites\", \"giant eagle #24 cafe\", \"giant eagle #67 cafe\",\n",
    "            \"giant eagle cafe\", \"giant eagle wexford cafe #45\",\n",
    "            \"starbucks temporary baggage claim kiosk\", 'giant eagle cafe #18',\n",
    "            'giant eagle cafe #52', 'giant eagle cafe #61', 'giant eagle cafe #068',\n",
    "            'giant eagle cafe #619', 'giant eagle cafe #43', 'giant eagle cafe #60',\n",
    "            'giant eagle cafe #646', 'giant eagle cafe #0004', 'giant eagle cafe #6379',\n",
    "            'revel and roost']\n",
    "\n",
    "for facility_name in fa_names:\n",
    "    where_to_drop = (viol_yelp_diff.facility_name == facility_name)\n",
    "    viol_yelp_diff.drop(\n",
    "        index=viol_yelp_diff[where_to_drop].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_to_drop = (viol_yelp_diff.facility_name == \"starbucks coffee\") & (\n",
    "    viol_yelp_diff.num == 1000) & (viol_yelp_diff.street == \"Airport Blvd\")\n",
    "viol_yelp_diff.drop(index=viol_yelp_diff[where_to_drop].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting more IDs of Restaurants of the Violation Dataset\n",
    "\n",
    "In the violation dataset, we previously mentioned that same restaurants can have names spelled differently. The previous processing did not find all of these restaurants in the violation dataset. The mapping between the violation and Yelp business datasets can help in finding more of these instances. This can be done by finding the restaurants with business ID mapped to more than one ID, i.e., a restaurant from Yelp dataset mapped to more than one restaurant from violation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the restaurants with one business ID mapped to more than one ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_ids = viol_yelp_diff[['id', 'business_id']]\n",
    "\n",
    "# group by business_id and count for each group the number of unique IDs\n",
    "bus_ids = bus_ids.groupby('business_id').id.nunique()\n",
    "bus_ids = bus_ids.reset_index()\n",
    "# focus on restaurants with business_id mapped to more than one ID\n",
    "bus_ids = bus_ids[bus_ids.id > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then adjust the IDs and business starting dates of those restaurants in violation dataset and in the mapping dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bus_id in bus_ids.business_id:\n",
    "    fc_ids = viol_yelp_diff[viol_yelp_diff.business_id == bus_id].id\n",
    "    # adjust the id and businsess starting dates for restaurants with more than one ID\n",
    "    violation.at[violation.id.isin(\n",
    "        fc_ids), 'bus_st_date'] = violation[violation.id.isin(fc_ids)].bus_st_date.min()\n",
    "    violation.at[violation.id.isin(fc_ids), 'id'] = fc_ids.min()\n",
    "    viol_yelp_diff.at[viol_yelp_diff.business_id ==\n",
    "                      bus_id, 'id'] = fc_ids.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the Business IDs of Some Restaurants of Yelp dataset\n",
    "\n",
    "We similarly check if same restaurants in the Yelp dataset have different business IDs.\n",
    "\n",
    "To find those restaurants, we check if there are restaurants with IDs mapped to more than one business IDs, i.e., a restaurant from violation dataset mapped to more than one restaurant from Yelp dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the restaurants with one ID mapped to more than one business ID, using the mapping dataframe \"viol_yelp_diff\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_ids = viol_yelp_diff[['id', 'business_id']]\n",
    "\n",
    "# group by id and count for each group the number of unique business IDs\n",
    "fc_ids = fc_ids.groupby('id').business_id.nunique()\n",
    "fc_ids = fc_ids.reset_index()\n",
    "\n",
    "# focus on restaurants with id mapped to more than one business ID\n",
    "fc_ids = fc_ids[fc_ids.business_id > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then adjust the business ID of those restaurants in the mappings dataframe and we also adjust the number of total reviews, the average number of stars and we merge the attributes and the categories of those restaurants in the business Yelp dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_ids_saved = []\n",
    "\n",
    "for fc_id in fc_ids.id:\n",
    "    # get the business ids of restaurants with more than one ID\n",
    "    bus_ids = viol_yelp_diff[viol_yelp_diff.id == fc_id].business_id\n",
    "    where_id = bus_df.business_id.isin(bus_ids)\n",
    "    \n",
    "    # compute the total review, total stars\n",
    "    # combine attribute and categories\n",
    "    total_reviews = bus_df[where_id].review_count.sum()\n",
    "    total_stars = (bus_df[where_id].stars *\n",
    "                   bus_df[where_id].review_count).sum()/total_reviews\n",
    "    attributes = bus_df[where_id].attributes\n",
    "    att_merged = dict()\n",
    "    for att in attributes:\n",
    "        if att is not None:\n",
    "            att_merged.update(att)\n",
    "    categories = bus_df[where_id].categories.str.cat()\n",
    "    \n",
    "    bus_ids_saved.append(list(bus_ids))\n",
    "    \n",
    "    # update business dataset\n",
    "    bus_df.at[where_id, 'stars'] = total_stars\n",
    "    bus_df.at[where_id, 'review_count'] = total_reviews\n",
    "    bus_df.at[where_id, 'attributes'] = json.dumps(att_merged)\n",
    "    bus_df.at[where_id, 'categories'] = categories\n",
    "    viol_yelp_diff.at[viol_yelp_diff.id ==\n",
    "                      fc_id, \"business_id\"] = bus_ids.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the restaurants with one ID mapped to more than one business ID, using the mapping dataframe \"viol_yelp_same\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_ids = viol_yelp_same[['id', 'business_id']]\n",
    "fc_ids = fc_ids.groupby('id').business_id.nunique()\n",
    "fc_ids = fc_ids.reset_index()\n",
    "fc_ids = fc_ids[fc_ids.business_id > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc_id in fc_ids.id:\n",
    "    bus_ids = viol_yelp_same[viol_yelp_same.id == fc_id].business_id\n",
    "    where_id = bus_df.business_id.isin(bus_ids)\n",
    "    total_reviews = bus_df[where_id].review_count.sum()\n",
    "    total_stars = (bus_df[where_id].stars *\n",
    "                   bus_df[where_id].review_count).sum()/total_reviews\n",
    "    attributes = bus_df[where_id].attributes\n",
    "    att_merged = dict()\n",
    "    for att in attributes:\n",
    "        if att is not None:\n",
    "            att_merged.update(att)\n",
    "    categories = bus_df[where_id].categories.str.cat()\n",
    "    bus_ids_saved.append(list(bus_ids))\n",
    "    bus_df.at[where_id, 'stars'] = total_stars\n",
    "    bus_df.at[where_id, 'review_count'] = total_reviews\n",
    "    bus_df.at[where_id, 'attributes'] = json.dumps(att_merged)\n",
    "    bus_df.at[where_id, 'categories'] = categories\n",
    "    viol_yelp_same.at[viol_yelp_same.id ==\n",
    "                      fc_id, \"business_id\"] = bus_ids.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_yelp_diff.to_csv(\"viol_yelp_diff_2.csv\")\n",
    "viol_yelp_same.to_csv(\"viol_yelp_same_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Final Mapping between the Datasets\n",
    "\n",
    "We finally extract the mapping between the violation IDs and business IDs in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = pd.concat([viol_yelp_same,viol_yelp_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = maps [['id','business_id']]\n",
    "maps = maps.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now merge the information provided by Yelp business dataset with the violation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_busid = violation.merge(maps,left_on=\"id\",right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_bus = violation_busid.merge(bus_df,left_on=\"business_id\",right_on=\"business_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps.to_csv(\"maps.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
