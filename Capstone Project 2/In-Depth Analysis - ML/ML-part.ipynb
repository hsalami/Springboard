{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-data\" data-toc-modified-id=\"Loading-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the data</a></span></li><li><span><a href=\"#Nested-Cross-Validation\" data-toc-modified-id=\"Nested-Cross-Validation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nested Cross-Validation</a></span></li><li><span><a href=\"#Encoding-the-Categorical-Variables\" data-toc-modified-id=\"Encoding-the-Categorical-Variables-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Encoding the Categorical Variables</a></span></li><li><span><a href=\"#Training-Regression-Models\" data-toc-modified-id=\"Training-Regression-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training Regression Models</a></span></li><li><span><a href=\"#Feature-Selection-with-Regression-Problem\" data-toc-modified-id=\"Feature-Selection-with-Regression-Problem-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Selection with Regression Problem</a></span></li><li><span><a href=\"#Final-Model-Tuning\" data-toc-modified-id=\"Final-Model-Tuning-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Final Model Tuning</a></span></li><li><span><a href=\"#Performance-of-Final-Regression-Models\" data-toc-modified-id=\"Performance-of-Final-Regression-Models-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Performance of Final Regression Models</a></span></li><li><span><a href=\"#Classification-Version-of-the-Problem\" data-toc-modified-id=\"Classification-Version-of-the-Problem-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Classification Version of the Problem</a></span></li><li><span><a href=\"#Training-Classification-Models\" data-toc-modified-id=\"Training-Classification-Models-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Training Classification Models</a></span></li><li><span><a href=\"#Feature-Selection-with-Classification\" data-toc-modified-id=\"Feature-Selection-with-Classification-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Feature Selection with Classification</a></span></li><li><span><a href=\"#Final-Model-Tuning\" data-toc-modified-id=\"Final-Model-Tuning-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Final Model Tuning</a></span></li><li><span><a href=\"#Performance-of-Final-Classification-Model\" data-toc-modified-id=\"Performance-of-Final-Classification-Model-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Performance of Final Classification Model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having explored the restaurants' characteristics and created some features that capture the historical performance of each restaurant, we now focus on building a predictive model that predicts the performance of a given restaurant during a future inspection. The prediction of future performances can be in terms of estimating the number of low, medium and high violations that can be detected during a future inspection, or in terms of classifying a future inspection as a fail or pass. In other words, our problem can be formulated as a regression or classification problem. While the regression problem provides more granularity to know which restaurants to check first (i.e., those with large high violations number), the data might not be rich enough to predict the number of each type of violation which will make the regression a difficult task with the data we have. In this report, we explore both approaches of the problem. \n",
    "\n",
    "The report is divided as follows: we first prepare the features columns and the target variables and split the data into training and test sets where the test set consist of future inspections for the training set. We then explain the cross-validation process that we are going to use in building the predictive models and in selecting the final model. We then start with the regression problem, pre-process the training set, perform feature selection, train some regression models, check their performance in terms of MSE (mean squared error) and R-squared and select a final model. We also repeat the same steps with the classification version of the problem. We finally finish with the insights provided by the models built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "We first import the required modules and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pingouin import anova\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score as accuracy, precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall, f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_to_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the columns of feature into X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['encounter', 'low', 'medium', 'high'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are first focusing on the regression problem, our target variables will be the number of low, medium and high violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[['low', 'medium', 'high']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into training and test sets: 80% for training and 20% for testing. Note that the test set corresponds to future inspections for the training set; the data is already sorted according to the inspection date, this is why the test set consists of the last 20% of the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X.iloc[0:int(len(data)*0.8),:]\n",
    "Ytrain = Y.iloc[0:int(len(data)*0.8),:]\n",
    "Xtest = X.iloc[int(len(data)*0.8):,:]\n",
    "Ytest = Y.iloc[int(len(data)*0.8):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross-Validation\n",
    "\n",
    "Before we start with the training step, we explain here the cross-validation method that we are going to use to perform feature selection and model selection. Since each instance in our training set corresponds to an inspection on a specific day and holds information about past violations, and since our goal is to predict future performance of restaurants, we are not going to use the regular cross-validation; when we are going to split the training set into sub-training and validations sets, the validation set should consist of subsequent instances for the sub-training set. In particular, we choose the first 60% of the training data as sub-training set and the next 20% of the training data as validation set, then we repeat the same process but now the first 70% of the data represents the sub-training set and the next 20% as validation set and then finally, we use the first 80% of the data as sub-training set and the remaining 20% as validation set. We then compute the average of the performances obtained with each validation set.\n",
    "\n",
    "We define the function of nested cross-validation as follows. The function takes as input the data features, the target variable, and the metric function that is used in evaluating the performance of the trained model. The function also includes the option of whether we want to scale the data or select from its features. The part of feature selection will be explained in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_score(x, y, clf, metric_func, scale=False, select=False,\n",
    "                    score_func=None):\n",
    "    result = np.zeros((3, len(metric_func)))\n",
    "    j = 0\n",
    "    for i in [0.6, 0.7, 0.8]:\n",
    "        x_train = x.iloc[0:int(len(x)*i), :]\n",
    "        y_train = y.iloc[0:int(len(y)*i)]\n",
    "        if(scale == True):\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "        if(select == True):\n",
    "            scores = score_func(x_train, y_train, random_state=300)\n",
    "            x_train = x_train[:, scores != 0]\n",
    "            print(\"Number of columns dropped:\", sum(scores == 0))\n",
    "\n",
    "        x_test = x.iloc[int(len(x)*i):int(len(x)*(i+0.2)), ]\n",
    "        x_test = x_test.values\n",
    "        if(scale == True):\n",
    "            x_test = scaler.transform(x_test)\n",
    "        if(select == True):\n",
    "            x_test = x_test[:, scores != 0]\n",
    "\n",
    "        y_test = y.iloc[int(len(x)*i):int(len(x)*(i+0.2))]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        ypred = clf.predict(x_test)\n",
    "        for func, ind in zip(metric_func, range(0, len(metric_func))):\n",
    "            result[j, ind] = func(y_test, ypred)\n",
    "        j = j+1\n",
    "\n",
    "    return(np.mean(result, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the Categorical Variables\n",
    "\n",
    "The data consist of numerical and categorical columns. The latter include both nominal and ordinal columns. We now encode the nominal categorical columns. \n",
    "\n",
    "The columns that we need to encode are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = [\n",
    "    'description', 'alcohol', 'good_for_kids', 'good_for_groups',\n",
    "    'price_range', 'casual', 'classy', 'divey', 'hipster', 'intimate',\n",
    "    'romantic', 'touristy', 'trendy', 'upscale', 'day_name', 'American',\n",
    "    'Italian', 'French', 'Mediterranean', 'Spanish', 'European', 'Mexican',\n",
    "    'Latin American', 'African', 'Caribbean', 'Southern', 'Japanese',\n",
    "    'Chinese', 'E Asian', 'N/C Asian', 'SE Asian', 'Indian', 'Australian',\n",
    "    'Pizza', 'Fast/Fried Foods', 'Burgers', 'Dessert', 'Bagels/Pretzels',\n",
    "    'Gelato', 'Seafood', 'BBQ', 'Steaks', 'Vegetarian', 'Vegan', 'Gluten-Free',\n",
    "    'Noodles', 'Tacos', 'Sandwiches', 'Sushi', 'Kosher', 'Fruit/Veg', 'Other',\n",
    "    'Restaurants', 'Mobile', 'Convenience Store', 'Grocery Store', 'Food Shop',\n",
    "    'Bakeries', 'Coffee Place', 'Other Goods', 'Shopping', 'Services',\n",
    "    'Entertainment/Event Place', 'Fitness/Sport Place',\n",
    "    'Teaching/School Place', 'Religious Place', 'Health & Medical Place',\n",
    "    'Pub/Bars', 'Liquor Manufacturing', 'zip', 'municipal'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try the classic one hot encoding and label encoding as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_onehot = ce.OneHotEncoder(cols=cols_to_encode).fit(Xtrain)\n",
    "Xtrain_onehot = enc_onehot.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_ord= ce.OrdinalEncoder(cols=cols_to_encode).fit(Xtrain)\n",
    "Xtrain_ord = enc_ord.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while most of the categorical columns have at most three unique values, the columns: description, zip and municipal contain more than 3 unique values (description: 21, zip: 88, municipal: 136). On hot encoding scheme increases in this case significantly the total number of features, which we are going to address later through feature elimination.  \n",
    "The ordinal columns consist of the following columns: 'month', 'day', 'year', 'hour', 'inspect_num' and they all have been assigned integer values, therefore they do not need any encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression Models\n",
    "\n",
    "We now start with training the data, using our defined nested cross-validation function. The models that we are going to use are: linear support vector regressor (used a baseline model) and tree based models: random forest, ada boost, and gradient boosting regressors. We first start with the default parameters for each model, we use the training data encoded with one hot and ordinal encodings, we also start with all features and compare between the models using mean-squared error and we will also look at their R-squared scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearSVR(random_state=1), RandomForestRegressor(random_state=1), \n",
    "          AdaBoostRegressor(random_state=1), \n",
    "          GradientBoostingRegressor(random_state=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Mean Squared Error  R2 Score\n",
      "One Hot Encoding - Low Violation                              \n",
      "Linear SVR                                  8.045280  0.008385\n",
      "Random Forest Regressor                     5.836208  0.281117\n",
      "AdaBoost Regressor                          9.030906 -0.112146\n",
      "Gradient Boosting Regressor                 5.843191  0.280142\n",
      "                                     Mean Squared Error  R2 Score\n",
      "One Hot Encoding - Medium Violation                              \n",
      "Linear SVR                                     1.458459  0.004655\n",
      "Random Forest Regressor                        1.246249  0.149597\n",
      "AdaBoost Regressor                             2.503747 -0.708556\n",
      "Gradient Boosting Regressor                    1.243516  0.151465\n",
      "                                   Mean Squared Error  R2 Score\n",
      "One Hot Encoding - High Violation                              \n",
      "Linear SVR                                   0.807403  0.024286\n",
      "Random Forest Regressor                      0.731271  0.112643\n",
      "AdaBoost Regressor                           1.822915 -1.213779\n",
      "Gradient Boosting Regressor                  0.690629  0.162729\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for viol in ['low', 'medium', 'high']:\n",
    "    results = []\n",
    "    for model in models:\n",
    "        res = nested_cv_score(Xtrain_onehot, Ytrain[viol], model,\n",
    "                              [mean_squared_error, r2_score], scale=True)\n",
    "        results.append(res)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = [\"Mean Squared Error\", \"R2 Score\"]\n",
    "    results.rename(index={0: \"Linear SVR\", 1: \"Random Forest Regressor\",\n",
    "                          2: \"AdaBoost Regressor\",\n",
    "                          3: \"Gradient Boosting Regressor\"}, inplace=True)\n",
    "    if viol == 'low':\n",
    "        results.index.name = \"One Hot Encoding - Low Violation\"\n",
    "    elif viol == 'medium':\n",
    "        results.index.name = \"One Hot Encoding - Medium Violation\"\n",
    "    else:\n",
    "        results.index.name = \"One Hot Encoding - High Violation\"\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For low and high violations, we see that Gradient boosting regressor and random forest have  better performance (in terms of lower MSE) than the other two models. With high violations, gradient boosting regressor showed the lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Mean Squared Error  R2 Score\n",
      "Ordinal Encoding - Low Violation                              \n",
      "Linear SVR                                  7.814561  0.036509\n",
      "Random Forest Regressor                     5.896790  0.273740\n",
      "AdaBoost Regressor                          8.236327 -0.015791\n",
      "Gradient Boosting Regressor                 5.834430  0.281412\n",
      "                                     Mean Squared Error  R2 Score\n",
      "Ordinal Encoding - Medium Violation                              \n",
      "Linear SVR                                     1.369467  0.065405\n",
      "Random Forest Regressor                        1.259151  0.140800\n",
      "AdaBoost Regressor                             2.270310 -0.549595\n",
      "Gradient Boosting Regressor                    1.246730  0.149238\n",
      "                                   Mean Squared Error  R2 Score\n",
      "Ordinal Encoding - High Violation                              \n",
      "Linear SVR                                   0.817994  0.010179\n",
      "Random Forest Regressor                      0.742043  0.099861\n",
      "AdaBoost Regressor                           1.476398 -0.788061\n",
      "Gradient Boosting Regressor                  0.699775  0.151502\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for viol in ['low', 'medium', 'high']:\n",
    "    results = []\n",
    "    for model in models:\n",
    "        res = nested_cv_score(Xtrain_ord, Ytrain[viol], model,\n",
    "                              [mean_squared_error, r2_score], scale=True)\n",
    "        results.append(res)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = [\"Mean Squared Error\", \"R2 Score\"]\n",
    "    results.rename(index={0: \"Linear SVR\", 1: \"Random Forest Regressor\",\n",
    "                          2: \"AdaBoost Regressor\",\n",
    "                          3: \"Gradient Boosting Regressor\"}, inplace=True)\n",
    "    if viol == 'low':\n",
    "        results.index.name = \"Ordinal Encoding - Low Violation\"\n",
    "    elif viol == 'medium':\n",
    "        results.index.name = \"Ordinal Encoding - Medium Violation\"\n",
    "    else:\n",
    "        results.index.name = \"Ordinal Encoding - High Violation\"\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this type of encoding, we notice that gradient boosting classifier always has the lowest MSE for all types of violations. We also notice that gradient boosting with ordinal encoding performed slightly better than with one hot encoding. On the other hand, with medium and high violations, one hot encoding performed better than ordinal encoding.\n",
    "\n",
    "However, if we look at the score of R-squared, we see that all models are not performing well in terms how well they are fitting the data. We next focus on gradient boosting model and perform feature selection to see if the performance can be boosted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with Regression Problem\n",
    "\n",
    "We now perform feature selection using the filter method. In particular, we compute the mutual information between each feature and the the number of violation for each type. We then drop the features that have 0 mutual information with the target variable. This can help us in removing the redundant features, especially with the one hot encoding scheme that significantly increases the total number of features. We chose the mutual information test over the F-test, because F-test requires some assumptions on the features of the data (normality assumption with same variance), while mutual information do not require that the features hold those assumptions. Moreover, mutual information can detect non-linear relationship between the features and the target variables.\n",
    "\n",
    "To test whether reducing the number of features can reduce the MSE of the models, we again perform cross-validation, where for each sub-training set we remove the features that have 0 mutual information with the target variable. We compare the obtained results with those obtained when the models considered the total number of features.\n",
    "\n",
    "We start with the data encoded with one hot encoding; the total number of features here is 499."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14648, 499)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_onehot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of columns dropped: 204\n",
      "Numbers of columns dropped: 200\n",
      "Numbers of columns dropped: 186\n",
      "                                  Mean Squared Error  R2 Score\n",
      "One Hot Encoding - Low Violation                              \n",
      "0                                           5.850644  0.279305\n",
      "\n",
      "\n",
      "Numbers of columns dropped: 214\n",
      "Numbers of columns dropped: 211\n",
      "Numbers of columns dropped: 211\n",
      "                                     Mean Squared Error  R2 Score\n",
      "One Hot Encoding - Medium Violation                              \n",
      "0                                               1.23661  0.156163\n",
      "\n",
      "\n",
      "Numbers of columns dropped: 189\n",
      "Numbers of columns dropped: 184\n",
      "Numbers of columns dropped: 208\n",
      "                                   Mean Squared Error  R2 Score\n",
      "One Hot Encoding - High Violation                              \n",
      "0                                            0.691962  0.161294\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for viol in ['low', 'medium', 'high']:\n",
    "    results = nested_cv_score(Xtrain_onehot, Ytrain[viol],\n",
    "                              GradientBoostingRegressor(random_state=1),\n",
    "                          [mean_squared_error, r2_score], scale=True, select=True,\n",
    "                          score_func=mutual_info_regression)\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    results = results.T\n",
    "    results.columns = [\"Mean Squared Error\", \"R2 Score\"]\n",
    "    if viol == 'low':\n",
    "        results.index.name = \"One Hot Encoding - Low Violation\"\n",
    "    elif viol == 'medium':\n",
    "        results.index.name = \"One Hot Encoding - Medium Violation\"\n",
    "    else:\n",
    "        results.index.name = \"One Hot Encoding - High Violation\"\n",
    "    print(results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare those results with the ones obtained from the total number of features, we notice that for low and high violations (with one hot encoding), the MSE and R2 scores are close (slightly better with the complete number of features), and with medium violations the MSE and R2 scores are better with less features. Let us now check the other encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of columns dropped: 36\n",
      "Numbers of columns dropped: 18\n",
      "Numbers of columns dropped: 26\n",
      "                                  Mean Squared Error  R2 Score\n",
      "One Hot Encoding - Low Violation                              \n",
      "0                                           5.836886  0.281165\n",
      "\n",
      "\n",
      "Numbers of columns dropped: 35\n",
      "Numbers of columns dropped: 26\n",
      "Numbers of columns dropped: 18\n",
      "                                     Mean Squared Error  R2 Score\n",
      "One Hot Encoding - Medium Violation                              \n",
      "0                                              1.247337  0.148848\n",
      "\n",
      "\n",
      "Numbers of columns dropped: 22\n",
      "Numbers of columns dropped: 16\n",
      "Numbers of columns dropped: 18\n",
      "                                   Mean Squared Error  R2 Score\n",
      "One Hot Encoding - High Violation                              \n",
      "0                                            0.699358  0.152308\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for viol in ['low', 'medium', 'high']:\n",
    "    results = nested_cv_score(Xtrain_ord, Ytrain[viol],\n",
    "                              GradientBoostingRegressor(random_state=1),\n",
    "                          [mean_squared_error, r2_score], scale=True, select=True,\n",
    "                          score_func=mutual_info_regression)\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    results = results.T\n",
    "    results.columns = [\"Mean Squared Error\", \"R2 Score\"]\n",
    "    if viol == 'low':\n",
    "        results.index.name = \"One Hot Encoding - Low Violation\"\n",
    "    elif viol == 'medium':\n",
    "        results.index.name = \"One Hot Encoding - Medium Violation\"\n",
    "    else:\n",
    "        results.index.name = \"One Hot Encoding - High Violation\"\n",
    "    print(results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that with low violations, performing ordinal encoding with feature selection gave the best MSE score. For medium violations, performing one hot encoding with feature selection gave the best MSE score. For high violations, performing one hot encoding without feature selection gave the best MSE score. We next focus on these combinations and fine-tune the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Tuning\n",
    "\n",
    "To check if we can obtain an additional boost in the performance of the gradient boosting model for each type of violation, we now focus on fine tuning some of its parameters. We are also going to use nested cross validation to find the best parameters, but instead of using our defined function for nested cross validation, we are going to divide the process; this is because mutual information takes some time to compute, we will first pre-process our training data for each type of violation and make it ready for model tuning.\n",
    "\n",
    "We first define two functions that we will use to prepare and pre-process the sub-training and validation sets for each step of the cross validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(x_train, y_train, x_test, select=False, scale=False, score_func=None):\n",
    "\n",
    "    if(scale == True):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "    if(select == True):\n",
    "        scores = score_func(x_train, y_train)\n",
    "        x_train = x_train[:, scores != 0]\n",
    "        x_test = x_test[:, scores != 0]\n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cross_validation(x, y, select=False):\n",
    "    res = []\n",
    "    for i in [0.6, 0.7, 0.8]:\n",
    "        x_train = x.iloc[0:int(len(x)*i), :]\n",
    "        y_train = y.iloc[0:int(len(y)*i)]\n",
    "        x_test = x.iloc[int(len(x)*i):int(len(x)*(i+0.2)), :]\n",
    "        y_test = y.iloc[int(len(x)*i):int(len(x)*(i+0.2))]\n",
    "        x_train, x_test = prepare(x_train, y_train, x_test, select=select,\n",
    "                                  scale=True, score_func=mutual_info_regression)\n",
    "        res.append(x_train)\n",
    "        res.append(y_train)\n",
    "        res.append(x_test)\n",
    "        res.append(y_test)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have defined the functions that prepare our training data for cross validation, we now use them to prepare the required datasets for each type of violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_data_low = prepare_cross_validation(Xtrain_ord, Ytrain['low'], select=True)\n",
    "xy_data_med = prepare_cross_validation(Xtrain_onehot, Ytrain['medium'], select=True)\n",
    "xy_data_high = prepare_cross_validation(Xtrain_onehot, Ytrain['high'], select=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the function that computes that average performances (in terms of MSE) of the given model (clf) on the validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_cv(xy_data, clf):\n",
    "    result = []\n",
    "    for i in [0, 1, 2]:\n",
    "        x_train = xy_data[0+4*(i)]\n",
    "        y_train = xy_data[1+4*(i)]\n",
    "        x_test = xy_data[2+4*(i)]\n",
    "        y_test = xy_data[3+4*(i)]\n",
    "        clf.fit(x_train, y_train)\n",
    "        ypred = clf.predict(x_test)\n",
    "        result.append(mean_squared_error(y_test, ypred))\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start with fine tuning the models. We will only focus on three parameters of gradient boosting regressor: \"n_estimators\", \"learning_rate\" and \"max_depth\". We start with the first two parameters \"n_estimators\"  and \"learning_rate\", because there is a trade-off between these two parameters (n_estimators is the total number of trees and learning rate is the coefficient that shrinks the contribution of each tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "250 0.05\n",
      "med\n",
      "100 0.05\n",
      "high\n",
      "250 0.05\n"
     ]
    }
   ],
   "source": [
    "n_est = [100, 250, 500]\n",
    "rates = [0.1, 0.05, 0.01]\n",
    "\n",
    "for viol in ['low', 'med', 'high']:\n",
    "    min_mse = 1000\n",
    "    for n in n_est:\n",
    "        for r in rates:\n",
    "            model = GradientBoostingRegressor(\n",
    "                random_state=1, n_estimators=n, learning_rate=r)\n",
    "\n",
    "            result = compute_mean_cv(\n",
    "                locals()['xy_data_{}'.format(viol)], model)\n",
    "            if(result < min_mse):\n",
    "                min_mse = result\n",
    "                min_n = n\n",
    "                min_r = r\n",
    "    print(viol)\n",
    "    print(min_mse, min_n, min_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is 0.05 for all models, and the number of trees is 100 for the model of medium violations and 250 for the models of medium and low violations. We now check the 'max_depth' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "5.790844846584156 4\n",
      "med\n",
      "1.2396162199120757 4\n",
      "high\n",
      "0.6914830138500928 3\n"
     ]
    }
   ],
   "source": [
    "depth = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for viol in ['low', 'med', 'high']:\n",
    "    min_mse = 1000\n",
    "    for d in depth:\n",
    "        n = 250\n",
    "        if (viol == 'med'):\n",
    "            n = 100\n",
    "        model = GradientBoostingRegressor(random_state=1, n_estimators=n,\n",
    "                                          learning_rate=0.05,\n",
    "                                          max_depth=d)\n",
    "        result = compute_mean_cv(locals()['xy_data_{}'.format(viol)], model)\n",
    "        if(result < min_mse):\n",
    "            min_mse = result\n",
    "            min_d = d\n",
    "    print(viol)\n",
    "    print(min_mse, min_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 as max_depth parameter for the models of low and medium violations, and 3 for the model of high violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next rebuild these models using the whole training set and check their performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Final Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the final models with their chosen parameters on the test set. For this sake, we first fit the chosen models on the whole training set and then check their performance using the test set.\n",
    "\n",
    "We first prepare and preprocess the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_onehot = enc_onehot.transform(Xtest)\n",
    "Xtest_ord = enc_ord.transform(Xtest)\n",
    "\n",
    "Xtrain_low, Xtest_low = prepare(Xtrain_ord, Ytrain['low'], Xtest_ord,\n",
    "                                select=True, scale=True,\n",
    "                                score_func=mutual_info_regression)\n",
    "Xtrain_med, Xtest_med = prepare(Xtrain_onehot, Ytrain['medium'],\n",
    "                                Xtest_onehot, select=True, scale=True,\n",
    "                                score_func=mutual_info_regression)\n",
    "Xtrain_high, Xtest_high = prepare(Xtrain_onehot, Ytrain['high'],\n",
    "                                  Xtest_onehot, select=True, scale=True,\n",
    "                                  score_func=mutual_info_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize the model for each violation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_low = GradientBoostingRegressor(random_state=1, n_estimators=250,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=4)\n",
    "model_med = GradientBoostingRegressor(random_state=1, n_estimators=100,\n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=4)\n",
    "model_high = GradientBoostingRegressor(random_state=1, n_estimators=250,\n",
    "                                       learning_rate=0.05,\n",
    "                                       max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the models using the whole training set and then check their performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_low.fit(Xtrain_low, Ytrain['low'])\n",
    "model_med.fit(Xtrain_med, Ytrain['medium'])\n",
    "model_high.fit(Xtrain_high, Ytrain['high'])\n",
    "\n",
    "ypred_low = model_low.predict(Xtest_low)\n",
    "ypred_med = model_med.predict(Xtest_med)\n",
    "ypred_high = model_high.predict(Xtest_high)\n",
    "\n",
    "mse_low = mean_squared_error(Ytest['low'], ypred_low)\n",
    "mse_med = mean_squared_error(Ytest['medium'], ypred_med)\n",
    "mse_high = mean_squared_error(Ytest['high'], ypred_high)\n",
    "\n",
    "rs_low = r2_score(Ytest['low'], ypred_low)\n",
    "rs_med = r2_score(Ytest['medium'], ypred_med)\n",
    "rs_high = r2_score(Ytest['high'], ypred_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Violations\n",
      "MSE: 5.832756214471463 R2: 0.24033434619115324\n",
      "Medium Violations\n",
      "MSE: 1.3046813152332875 R2: 0.15682566866445458\n",
      "High Violations\n",
      "MSE: 0.7639406850414634 R2: 0.12615197445631632\n"
     ]
    }
   ],
   "source": [
    "print('Low Violations')\n",
    "print('MSE: {}'.format(mse_low), 'R2: {}'.format(rs_low))\n",
    "print('Medium Violations')\n",
    "print('MSE: {}'.format(mse_med), 'R2: {}'.format(rs_med))\n",
    "print('High Violations')\n",
    "print('MSE: {}'.format(mse_high), 'R2: {}'.format(rs_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the models do not fit well the data, which suggests that regression here is a difficult task with the data we have. Let us know try the second approach of the problem: classification, to see if we can instead detect the set of restaurants that might fail a future inspection (instead of estimating the possible number of violations that can be detected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Version of the Problem\n",
    "\n",
    "We now focus on the classification version of the problem. Since estimating the number of possible violations turned out to be a difficult task for our data, detecting the set of possible restaurants that could fail a future inspection might be an easier task. We now explore the classification approach. By failing an inspection, we assume that at least one high violation or at least 2 medium violations were detected during the inspection. Using this assumption, we create the target variable so that '1' means 'Failed' and '0' means 'Passed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.Series(len(X)*[0])\n",
    "Y[np.logical_or((data.medium>1),(data.high>0))]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the proportion of each class to verify if there is any imbalance between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Passes: 51.848615586259626\n",
      "Percentage of Failures: 48.151384413740374\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of Passes:\", 100*sum(Y==0)/len(Y))\n",
    "print(\"Percentage of Failures:\", 100*sum(Y==1)/len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportions of the two classes are close to each other, we do not have problems with class imbalances. We now create the training and testing sets for the labeled target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = Y.iloc[0:int(len(data)*0.8)]\n",
    "Ytest = Y.iloc[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next train some regression problems, perform again feature selection and then fine tune the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare between the models, we again use our nested cross validation function and the performance is evaluated based on the average accuracy of the models. The models we are going to train are: logistic regression (used as baseline), and tree based classifiers (random fores, ada boost and gradient boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SGDClassifier(loss='log', random_state=1), \n",
    "          RandomForestClassifier(random_state=1),\n",
    "          AdaBoostClassifier(random_state=1), \n",
    "          GradientBoostingClassifier(random_state=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start with the data processed with one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Accuracy\n",
      "One Hot Encoding                      \n",
      "Logistic Regression           0.586462\n",
      "Random Forest Classifier      0.659954\n",
      "AdaBoost Classifier           0.658362\n",
      "Gradient Boosting Classifier  0.673834\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for model in models:\n",
    "    res = nested_cv_score(Xtrain_onehot, Ytrain, model, [accuracy], scale=True)\n",
    "    results.append(res)\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = [\"Accuracy\"]\n",
    "results.rename(index={0: \"Logistic Regression\", 1: \"Random Forest Classifier\", \n",
    "                      2: \"AdaBoost Classifier\",\n",
    "                     3: \"Gradient Boosting Classifier\"}, inplace=True)\n",
    "results.index.name = \"One Hot Encoding\" \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that gradient boosting classifier also performed the best between the models in terms of accuracy. Let us check now with label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Accuracy\n",
      "Ordinal Encoding                      \n",
      "Logistic Regression           0.617292\n",
      "Random Forest Classifier      0.665529\n",
      "AdaBoost Classifier           0.662344\n",
      "Gradient Boosting Classifier  0.674858\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for model in models:\n",
    "    res = nested_cv_score(Xtrain_ord, Ytrain, model, [accuracy], scale=True)\n",
    "    results.append(res)\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = [\"Accuracy\"]\n",
    "results.rename(index={0: \"Logistic Regression\", 1: \"Random Forest Classifier\", \n",
    "                      2: \"AdaBoost Classifier\",\n",
    "                      3: \"Gradient Boosting Classifier\"}, inplace=True)\n",
    "results.index.name = \"Ordinal Encoding\" \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With label encoding, we obtained similar accuracy scores. We next focus only on gradient boosting classifier and check if reducing the number of features can enhance the accuracy scores of the model. We check both encoding schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with Classification\n",
    "\n",
    "To perform feature selection, we again use the filter method with the score function mutual information.\n",
    "\n",
    "We start with one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped: 200\n",
      "Number of columns dropped: 196\n",
      "Number of columns dropped: 181\n",
      "accuracy [0.66916951]\n"
     ]
    }
   ],
   "source": [
    "result = nested_cv_score(Xtrain_onehot, Ytrain,\n",
    "                          GradientBoostingClassifier(random_state=1),\n",
    "                          [accuracy], scale=True, select=True,\n",
    "                          score_func=mutual_info_classif)\n",
    "print(\"accuracy\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy is slightly less than what we obtained with one hot encoding using the total number of features (0.674). Let us know check with ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped: 17\n",
      "Number of columns dropped: 12\n",
      "Number of columns dropped: 12\n",
      "accuracy [0.67360637]\n"
     ]
    }
   ],
   "source": [
    "result = nested_cv_score(Xtrain_ord, Ytrain,\n",
    "                          GradientBoostingClassifier(random_state=1),\n",
    "                          [accuracy], scale=True, select=True,\n",
    "                          score_func=mutual_info_classif)\n",
    "print(\"accuracy\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we obtained here is also close to what we obtained with ordinal encoding using the complete number of features. However, since the accuracy is better with ordinal encoding without feature selection, we are going next to focus on this combination (ordinal encoding without feature selection) to fine-tune the gradient boosting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Tuning\n",
    "\n",
    "After having chosen the combination of label encoding and gradient boosting classifier with no feature selection, we now focus on fine-tuning some of the parameters of the gradient boosting classifier. The parameters are: \"n_estimators\", \"learning_rate\" and \"max_depth\". We start with the first two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67622298] 500 0.05\n"
     ]
    }
   ],
   "source": [
    "n_est = [100, 250, 500]\n",
    "rates = [0.1, 0.05, 0.01]\n",
    "\n",
    "max_acc = 0\n",
    "for n in n_est:\n",
    "    for r in rates:\n",
    "        model = GradientBoostingClassifier(\n",
    "            random_state=1, n_estimators=n, learning_rate=r)\n",
    "        result = nested_cv_score(Xtrain_ord, Ytrain, model,\n",
    "                          [accuracy], scale=True, select=False)\n",
    "        if(result > max_acc):\n",
    "            max_acc = result\n",
    "            max_n = n\n",
    "            max_r = r\n",
    "print(max_acc, max_n, max_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain 500 for n_estimators and 0.05 for learning_rate. We now fine-tune the parameter \"max_depth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67622298] 3\n"
     ]
    }
   ],
   "source": [
    "depth = [2,3,4,5,6,7]\n",
    "\n",
    "max_acc = 0\n",
    "for d in depth:\n",
    "    model = GradientBoostingClassifier(random_state=1, n_estimators=max_n, \n",
    "                                       learning_rate=max_r, max_depth=d)\n",
    "    result =  nested_cv_score(Xtrain_ord, Ytrain, model,\n",
    "                              [accuracy], scale=True, select=False)\n",
    "    if(result > max_acc):\n",
    "        max_acc = result\n",
    "        max_d = d\n",
    "        \n",
    "print(max_acc, max_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain 3 for max_depth. We are now ready to train our final model on the whole training set and check its performance on our testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Final Classification Model\n",
    "\n",
    "To test our final model with its chosen parameters on the testing set, we first pre-process  the whole training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_ord = enc_ord.transform(Xtest)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain_ord)\n",
    "Xtrain_f = scaler.transform(Xtrain_ord)\n",
    "Xtest_f = scaler.transform(Xtest_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the gradient boosting classifier as our final model using the parameters we fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f= GradientBoostingClassifier(random_state=1, n_estimators=max_n, \n",
    "                                       learning_rate=max_r, max_depth=max_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model using the whole training set and check its performance on the test set, in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6764946764946765\n"
     ]
    }
   ],
   "source": [
    "model_f.fit(Xtrain_f,Ytrain)\n",
    "ypred = model_f.predict(Xtest_f)        \n",
    "print(\"Accuracy:\", accuracy(Ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is 67.65%, which is expected (the score is close to the ones we obtained when we have used cross validation on the training set). Let us look at the feature importances provided by the final model. We plot the features importance of the top 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(model_f.feature_importances_,\n",
    "          index=Xtrain_ord.columns).nlargest(10).plot(kind='barh')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Top 10 Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the most important feature is \"days_between\" which specifies the number of days that were between two consecutive inspections. We also notice that the month of the inspection is another important feature, which suggests the presence of some seasonal impacts that affect the restaurant performance. Moreover, past violations in terms of maximum number of high violations so far detected and the last number of high violations detected at a given restaurant are in the top 10 features, which suggests the importance of historical performance of restaurants. The topics modeled from the reviews (V3 and V4) also contributed to the performance of the final model. The feature \"Restaurants\" specifies if the inspected place is a restaurant or another type of food facility, this indicator represents also an important feature as we have already noticed from visualizing our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how the model is performing in predicting the true and negative labels, we plot the confusion matrix and compute the precision and recall scores for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model_f, Xtest_f, Ytest, normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 1 (Fail)\n",
      "Precision: 0.6726330664154498\n",
      "Recall 0.7445255474452555\n"
     ]
    }
   ],
   "source": [
    "print(\"Class: 1 (Fail)\")\n",
    "print(\"Precision:\", precision(Ytest,ypred))\n",
    "print(\"Recall\",recall(Ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 (Pass)\n",
      "Precision: 0.6818181818181818\n",
      "Recall: 0.6017191977077364\n"
     ]
    }
   ],
   "source": [
    "print(\"Class: 0 (Pass)\")\n",
    "print(\"Precision:\", precision(Ytest,ypred,pos_label=0))\n",
    "print(\"Recall:\",recall(Ytest,ypred,pos_label=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model is able to recall 74% of the failed inspections (recall for class 1: 74%), the precision for class 1 is 67% which represents the ratio of how many of the predicted failed inspections did actually fail. We have two goals for the inspectors: the first goal is to not let them miss inspecting the restaurants that are making serious violations, the second goal is to help the inspectors prioritize their efforts and be efficient in inspecting first the restaurants that will fail. The first goal can be measured by the precision score, while the second goal can be measured by the recall score. With the current information in our data, we were not able to get an accuracy above 67%; this suggests that if we need to get a better accuracy for our model (more instances of both classes correctly classified, more positive instances that are correctly classified enhance the recall score, more negative instances that are correctly classified enhance the precision score), more historical data need to be collected and more information about each restaurant need to specified. For instance, information related to the workers in the restaurants (number of employees, their level of expertise, their culinary background), or information related to the most times at which each restaurant is busy might all be helpful. Also many of the attributes and categories for many of the restaurants were not specified and were left empty, having them specified might also be helpful. Moreover, when we looked at the reviews we noticed that some restaurants did not have reviews for some periods of time, and we also noticed that even if the restaurant is providing satisfactory services for its customers, it can still make violations. Therefore, encouraging  people to leave more reviews that are not only related to the food served but also to the level of cleanness noticed in the restaurants (restrooms, utensils, quality of food served in terms of freshness) can also be helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
